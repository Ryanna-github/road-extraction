{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "import torchvision\n",
    "from torchvision import models\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, c_in, scale):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        assert scale in [1, 2, 4, 8]\n",
    "\n",
    "        if scale >= 1:\n",
    "            self.conv1 = Conv2dBN(c_in, c_in, 3, padding=1)\n",
    "        if scale >= 4:\n",
    "            self.conv2 = Conv2dBN(c_in, c_in, 3, padding=1)\n",
    "        if scale >= 8:\n",
    "            self.conv3 = Conv2dBN(c_in, c_in, 3, padding=1)\n",
    "\n",
    "        self.scale = scale\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.scale >= 1:\n",
    "            x = self.conv1(x)\n",
    "            if self.scale == 1:\n",
    "                return x\n",
    "\n",
    "        if self.scale >= 2:\n",
    "            x = F.interpolate(x, scale_factor=2, mode=\"bilinear\", align_corners=True)\n",
    "\n",
    "        if self.scale >= 4:\n",
    "            x = self.conv2(x)\n",
    "            x = F.interpolate(x, scale_factor=2, mode=\"bilinear\", align_corners=True)\n",
    "\n",
    "        if self.scale >= 8:\n",
    "            x = self.conv3(x)\n",
    "            x = F.interpolate(x, scale_factor=2, mode=\"bilinear\", align_corners=True)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FSModule(nn.Module):\n",
    "    def __init__(self, cv, cu):\n",
    "        super(FSModule, self).__init__()\n",
    "\n",
    "        self.conv1 = Conv2dBN(cv, cu, 1)\n",
    "        self.conv2 = Conv2dBN(cv, cu, 1)\n",
    "\n",
    "    def forward(self, v, u):\n",
    "        x = self.conv1(v)\n",
    "        r = torch.mul(x, u)\n",
    "        k = self.conv2(v)\n",
    "        z = k / (1 + torch.exp(-r))\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2dBN(nn.Module):\n",
    "    def __init__(self, c_in, c_out, filter_size, stride=1, padding=0, **kwargs):\n",
    "        super(Conv2dBN, self).__init__()\n",
    "        self.conv = nn.Conv2d(c_in, c_out, filter_size, stride=stride, padding=padding, **kwargs)\n",
    "        self.bn = nn.BatchNorm2d(c_out)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.relu(self.bn(self.conv(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_miou(pred_batch, label_batch, num_classes, ignore_index):\n",
    "    '''\n",
    "    :param pred_batch: [b,h,w]\n",
    "    :param label_batch: [b,h,w]\n",
    "    :param num_classes: scalar\n",
    "    :param ignore_index: scalar\n",
    "    :return:\n",
    "    '''\n",
    "    miou_sum, miou_count = 0, 0\n",
    "    for batch_idx in range(label_batch.shape[0]):\n",
    "        pred, label = pred_batch[batch_idx].flatten(0), label_batch[batch_idx].flatten(0)\n",
    "\n",
    "        mask = label != ignore_index\n",
    "        pred, label = pred[mask], label[mask]\n",
    "\n",
    "        pred_one_hot = F.one_hot(pred, num_classes)\n",
    "        label_one_hot = F.one_hot(label, num_classes)\n",
    "\n",
    "        intersection = torch.sum(pred_one_hot * label_one_hot)\n",
    "        union = torch.sum(pred_one_hot) + torch.sum(label_one_hot) - intersection + 1e-6\n",
    "\n",
    "        miou_sum += intersection / union\n",
    "        miou_count += 1\n",
    "    return miou_sum / miou_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FarSegNet(nn.Module):\n",
    "    def __init__(self, num_classes = 1, num_feature = 256, pretrained = False, ignore_index = 500):\n",
    "        super(FarSegNet, self).__init__()\n",
    "        \n",
    "        self.num_classes = num_classes\n",
    "        self.num_feature = num_feature\n",
    "        self.ignore_index = ignore_index # ignore losses of this class\n",
    "        self.EPS = 1e-5\n",
    "        self.current_step = 0\n",
    "        self.annealing_step = 2000\n",
    "        self.focal_factor = 4\n",
    "        self.focal_z = 1.0\n",
    "        \n",
    "        self.backbone = torchvision.models.resnet50(pretrained=True)\n",
    "        self.backbone_layer_c2 = nn.Sequential(*list(self.backbone.children())[:5])\n",
    "        self.backbone_layer_c3 = list(self.backbone.children())[5]\n",
    "        self.backbone_layer_c4 = list(self.backbone.children())[6]\n",
    "        self.backbone_layer_c5 = list(self.backbone.children())[7]\n",
    "        \n",
    "        self.conv_c6 = nn.Conv2d(2048, num_feature, 1)\n",
    "        self.conv_c5 = nn.Conv2d(2048, num_feature, 1)\n",
    "        self.conv_c4 = nn.Conv2d(1024, num_feature, 1)\n",
    "        self.conv_c3 = nn.Conv2d(512, num_feature, 1)\n",
    "        self.conv_c2 = nn.Conv2d(256, num_feature, 1)\n",
    "\n",
    "        self.fs5 = FSModule(num_feature, num_feature)\n",
    "        self.fs4 = FSModule(num_feature, num_feature)\n",
    "        self.fs3 = FSModule(num_feature, num_feature)\n",
    "        self.fs2 = FSModule(num_feature, num_feature)\n",
    "\n",
    "        self.up5 = Decoder(num_feature, 8)\n",
    "        self.up4 = Decoder(num_feature, 4)\n",
    "        self.up3 = Decoder(num_feature, 2)\n",
    "        self.up2 = Decoder(num_feature, 1)\n",
    "\n",
    "        self.classify = nn.Conv2d(num_feature, num_classes, 3, padding=1)\n",
    "    \n",
    "    def forward(self, x, label = None):\n",
    "        c2 = self.backbone_layer_c2(x)\n",
    "        c3 = self.backbone_layer_c3(c2)\n",
    "        c4 = self.backbone_layer_c4(c3)\n",
    "        c5 = self.backbone_layer_c5(c4)\n",
    "        c6 = F.adaptive_avg_pool2d(c5, (1, 1))\n",
    "        u = self.conv_c6(c6)\n",
    "        \n",
    "        p5 = self.conv_c5(c5)\n",
    "        p4 = (self.conv_c4(c4) + F.interpolate(p5, scale_factor = 2)) / 2.\n",
    "        p3 = (self.conv_c3(c3) + F.interpolate(p4, scale_factor = 2)) / 2.\n",
    "        p2 = (self.conv_c2(c2) + F.interpolate(p3, scale_factor = 2)) / 2.\n",
    "        \n",
    "        z5 = self.fs5(p5, u)\n",
    "        z4 = self.fs4(p4, u)\n",
    "        z3 = self.fs3(p3, u)\n",
    "        z2 = self.fs2(p2, u)\n",
    "\n",
    "        o5 = self.up5(z5)\n",
    "        o4 = self.up4(z4)\n",
    "        o3 = self.up3(z3)\n",
    "        o2 = self.up2(z2)\n",
    "        \n",
    "        x = (o5 + o4 + o3 + o2) / 4.\n",
    "        x = F.interpolate(x, scale_factor=4, mode=\"bilinear\", align_corners=True)\n",
    "        logit = self.classify(x)\n",
    "        \n",
    "        print(\"c2\\t\", c2.shape)\n",
    "        print(\"c3\\t\", c3.shape)\n",
    "        print(\"c4\\t\", c4.shape)\n",
    "        print(\"c5\\t\", c5.shape)\n",
    "        print(\"c6\\t\", c6.shape)\n",
    "        print(\"u\\t\", u.shape)\n",
    "        print('---------------------------------------------')\n",
    "        print(\"p5\\t\", p5.shape)\n",
    "        print(\"p4\\t\", p4.shape)\n",
    "        print(\"p3\\t\", p3.shape)\n",
    "        print(\"p2\\t\", p2.shape)\n",
    "        print('---------------------------------------------')\n",
    "        print(\"z5\\t\", z5.shape)\n",
    "        print(\"z4\\t\", z4.shape)\n",
    "        print(\"z3\\t\", z3.shape)\n",
    "        print(\"z2\\t\", z2.shape)\n",
    "        print('---------------------------------------------')\n",
    "        print(\"o5\\t\", o5.shape)\n",
    "        print(\"o4\\t\", o4.shape)\n",
    "        print(\"o3\\t\", o3.shape)\n",
    "        print(\"o2\\t\", o2.shape)\n",
    "        print('---------------------------------------------')\n",
    "        print(\"x\\t\", x.shape)\n",
    "        print(\"logit\\t\", logit.shape)\n",
    "        \n",
    "        if self.training:\n",
    "            return self._get_loss(logit, label), self._get_miou(logit, label)\n",
    "#         else:\n",
    "#             score_map = torch.softmax(logit, dim=1)\n",
    "#             score_map = score_map.permute(0, 2, 3, 1)\n",
    "#             pred = torch.argmax(score_map, dim=3)\n",
    "#             pred = torch.unsqueeze(pred, dim=3)\n",
    "#             return pred, score_map\n",
    "        \n",
    "    def _get_loss(self, logit, label):\n",
    "        \n",
    "        logit = logit.permute(0, 2, 3, 1).flatten()\n",
    "        label = label.flatten()\n",
    "        mask = label != self.ignore_index\n",
    "        logit, label = logit[mask], label[mask]\n",
    "        loss = nn.BCEWithLogitsLoss()(logit, label)\n",
    "\n",
    "        probs = torch.logit(logit)\n",
    "        p = ((1-label) + (-1)**(1+label)*probs).squeeze()\n",
    "\n",
    "        z = torch.pow(1.0 - p, self.focal_factor)\n",
    "        z = self.focal_z * z\n",
    "\n",
    "        if self.current_step < self.annealing_step:\n",
    "            z = z + (1 - z) * (1 - self.current_step / self.annealing_step)\n",
    "        self.current_step += 1\n",
    "\n",
    "        loss = z * loss\n",
    "        avg_loss = torch.mean(loss) / (torch.mean(mask.type(torch.float32)) + self.EPS)\n",
    "        return avg_loss\n",
    "\n",
    "    def _get_miou(self, logit, label):\n",
    "        pred = torch.argmax(logit, dim=1).squeeze_(dim=1)\n",
    "        return calc_miou(pred, label, self.num_classes, self.ignore_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = FarSegNet().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c2\t torch.Size([1, 256, 64, 64])\n",
      "c3\t torch.Size([1, 512, 32, 32])\n",
      "c4\t torch.Size([1, 1024, 16, 16])\n",
      "c5\t torch.Size([1, 2048, 8, 8])\n",
      "c6\t torch.Size([1, 2048, 1, 1])\n",
      "u\t torch.Size([1, 256, 1, 1])\n",
      "---------------------------------------------\n",
      "p5\t torch.Size([1, 256, 8, 8])\n",
      "p4\t torch.Size([1, 256, 16, 16])\n",
      "p3\t torch.Size([1, 256, 32, 32])\n",
      "p2\t torch.Size([1, 256, 64, 64])\n",
      "---------------------------------------------\n",
      "z5\t torch.Size([1, 256, 8, 8])\n",
      "z4\t torch.Size([1, 256, 16, 16])\n",
      "z3\t torch.Size([1, 256, 32, 32])\n",
      "z2\t torch.Size([1, 256, 64, 64])\n",
      "---------------------------------------------\n",
      "o5\t torch.Size([1, 256, 64, 64])\n",
      "o4\t torch.Size([1, 256, 64, 64])\n",
      "o3\t torch.Size([1, 256, 64, 64])\n",
      "o2\t torch.Size([1, 256, 64, 64])\n",
      "---------------------------------------------\n",
      "x\t torch.Size([1, 256, 256, 256])\n",
      "logit\t torch.Size([1, 1, 256, 256])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "one_hot is only applicable to index tensor.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-203-85600b24452f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m256\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-201-aa5966b9458d>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, label)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_miou\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0;31m#         else:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;31m#             score_map = torch.softmax(logit, dim=1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-201-aa5966b9458d>\u001b[0m in \u001b[0;36m_get_miou\u001b[0;34m(self, logit, label)\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_miou\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcalc_miou\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-17-8f93eff64ffb>\u001b[0m in \u001b[0;36mcalc_miou\u001b[0;34m(pred_batch, label_batch, num_classes, ignore_index)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mpred_one_hot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mone_hot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mlabel_one_hot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mone_hot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mintersection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_one_hot\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlabel_one_hot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: one_hot is only applicable to index tensor."
     ]
    }
   ],
   "source": [
    "x = torch.rand((1, 3, 256 ,256)).cuda()\n",
    "label = torch.ones((1, 1, 256 ,256)).cuda()\n",
    "               \n",
    "net(x, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c2\t torch.Size([2, 256, 64, 64])\n",
      "c3\t torch.Size([2, 512, 32, 32])\n",
      "c4\t torch.Size([2, 1024, 16, 16])\n",
      "c5\t torch.Size([2, 2048, 8, 8])\n",
      "c6\t torch.Size([2, 2048, 1, 1])\n",
      "u\t torch.Size([2, 256, 1, 1])\n",
      "---------------------------------------------\n",
      "p5\t torch.Size([2, 256, 8, 8])\n",
      "p4\t torch.Size([2, 256, 16, 16])\n",
      "p3\t torch.Size([2, 256, 32, 32])\n",
      "p2\t torch.Size([2, 256, 64, 64])\n",
      "---------------------------------------------\n",
      "z5\t torch.Size([2, 256, 8, 8])\n",
      "z4\t torch.Size([2, 256, 16, 16])\n",
      "z3\t torch.Size([2, 256, 32, 32])\n",
      "z2\t torch.Size([2, 256, 64, 64])\n",
      "---------------------------------------------\n",
      "o5\t torch.Size([2, 256, 64, 64])\n",
      "o4\t torch.Size([2, 256, 64, 64])\n",
      "o3\t torch.Size([2, 256, 64, 64])\n",
      "o2\t torch.Size([2, 256, 64, 64])\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 128, 128]           9,408\n",
      "            Conv2d-2         [-1, 64, 128, 128]           9,408\n",
      "       BatchNorm2d-3         [-1, 64, 128, 128]             128\n",
      "       BatchNorm2d-4         [-1, 64, 128, 128]             128\n",
      "              ReLU-5         [-1, 64, 128, 128]               0\n",
      "              ReLU-6         [-1, 64, 128, 128]               0\n",
      "         MaxPool2d-7           [-1, 64, 64, 64]               0\n",
      "         MaxPool2d-8           [-1, 64, 64, 64]               0\n",
      "            Conv2d-9           [-1, 64, 64, 64]           4,096\n",
      "           Conv2d-10           [-1, 64, 64, 64]           4,096\n",
      "      BatchNorm2d-11           [-1, 64, 64, 64]             128\n",
      "      BatchNorm2d-12           [-1, 64, 64, 64]             128\n",
      "             ReLU-13           [-1, 64, 64, 64]               0\n",
      "             ReLU-14           [-1, 64, 64, 64]               0\n",
      "           Conv2d-15           [-1, 64, 64, 64]          36,864\n",
      "           Conv2d-16           [-1, 64, 64, 64]          36,864\n",
      "      BatchNorm2d-17           [-1, 64, 64, 64]             128\n",
      "      BatchNorm2d-18           [-1, 64, 64, 64]             128\n",
      "             ReLU-19           [-1, 64, 64, 64]               0\n",
      "             ReLU-20           [-1, 64, 64, 64]               0\n",
      "           Conv2d-21          [-1, 256, 64, 64]          16,384\n",
      "           Conv2d-22          [-1, 256, 64, 64]          16,384\n",
      "      BatchNorm2d-23          [-1, 256, 64, 64]             512\n",
      "      BatchNorm2d-24          [-1, 256, 64, 64]             512\n",
      "           Conv2d-25          [-1, 256, 64, 64]          16,384\n",
      "           Conv2d-26          [-1, 256, 64, 64]          16,384\n",
      "      BatchNorm2d-27          [-1, 256, 64, 64]             512\n",
      "      BatchNorm2d-28          [-1, 256, 64, 64]             512\n",
      "             ReLU-29          [-1, 256, 64, 64]               0\n",
      "             ReLU-30          [-1, 256, 64, 64]               0\n",
      "       Bottleneck-31          [-1, 256, 64, 64]               0\n",
      "       Bottleneck-32          [-1, 256, 64, 64]               0\n",
      "           Conv2d-33           [-1, 64, 64, 64]          16,384\n",
      "           Conv2d-34           [-1, 64, 64, 64]          16,384\n",
      "      BatchNorm2d-35           [-1, 64, 64, 64]             128\n",
      "      BatchNorm2d-36           [-1, 64, 64, 64]             128\n",
      "             ReLU-37           [-1, 64, 64, 64]               0\n",
      "             ReLU-38           [-1, 64, 64, 64]               0\n",
      "           Conv2d-39           [-1, 64, 64, 64]          36,864\n",
      "           Conv2d-40           [-1, 64, 64, 64]          36,864\n",
      "      BatchNorm2d-41           [-1, 64, 64, 64]             128\n",
      "      BatchNorm2d-42           [-1, 64, 64, 64]             128\n",
      "             ReLU-43           [-1, 64, 64, 64]               0\n",
      "             ReLU-44           [-1, 64, 64, 64]               0\n",
      "           Conv2d-45          [-1, 256, 64, 64]          16,384\n",
      "           Conv2d-46          [-1, 256, 64, 64]          16,384\n",
      "      BatchNorm2d-47          [-1, 256, 64, 64]             512\n",
      "      BatchNorm2d-48          [-1, 256, 64, 64]             512\n",
      "             ReLU-49          [-1, 256, 64, 64]               0\n",
      "             ReLU-50          [-1, 256, 64, 64]               0\n",
      "       Bottleneck-51          [-1, 256, 64, 64]               0\n",
      "       Bottleneck-52          [-1, 256, 64, 64]               0\n",
      "           Conv2d-53           [-1, 64, 64, 64]          16,384\n",
      "           Conv2d-54           [-1, 64, 64, 64]          16,384\n",
      "      BatchNorm2d-55           [-1, 64, 64, 64]             128\n",
      "      BatchNorm2d-56           [-1, 64, 64, 64]             128\n",
      "             ReLU-57           [-1, 64, 64, 64]               0\n",
      "             ReLU-58           [-1, 64, 64, 64]               0\n",
      "           Conv2d-59           [-1, 64, 64, 64]          36,864\n",
      "           Conv2d-60           [-1, 64, 64, 64]          36,864\n",
      "      BatchNorm2d-61           [-1, 64, 64, 64]             128\n",
      "      BatchNorm2d-62           [-1, 64, 64, 64]             128\n",
      "             ReLU-63           [-1, 64, 64, 64]               0\n",
      "             ReLU-64           [-1, 64, 64, 64]               0\n",
      "           Conv2d-65          [-1, 256, 64, 64]          16,384\n",
      "           Conv2d-66          [-1, 256, 64, 64]          16,384\n",
      "      BatchNorm2d-67          [-1, 256, 64, 64]             512\n",
      "      BatchNorm2d-68          [-1, 256, 64, 64]             512\n",
      "             ReLU-69          [-1, 256, 64, 64]               0\n",
      "             ReLU-70          [-1, 256, 64, 64]               0\n",
      "       Bottleneck-71          [-1, 256, 64, 64]               0\n",
      "       Bottleneck-72          [-1, 256, 64, 64]               0\n",
      "           Conv2d-73          [-1, 128, 64, 64]          32,768\n",
      "           Conv2d-74          [-1, 128, 64, 64]          32,768\n",
      "      BatchNorm2d-75          [-1, 128, 64, 64]             256\n",
      "      BatchNorm2d-76          [-1, 128, 64, 64]             256\n",
      "             ReLU-77          [-1, 128, 64, 64]               0\n",
      "             ReLU-78          [-1, 128, 64, 64]               0\n",
      "           Conv2d-79          [-1, 128, 32, 32]         147,456\n",
      "           Conv2d-80          [-1, 128, 32, 32]         147,456\n",
      "      BatchNorm2d-81          [-1, 128, 32, 32]             256\n",
      "      BatchNorm2d-82          [-1, 128, 32, 32]             256\n",
      "             ReLU-83          [-1, 128, 32, 32]               0\n",
      "             ReLU-84          [-1, 128, 32, 32]               0\n",
      "           Conv2d-85          [-1, 512, 32, 32]          65,536\n",
      "           Conv2d-86          [-1, 512, 32, 32]          65,536\n",
      "      BatchNorm2d-87          [-1, 512, 32, 32]           1,024\n",
      "      BatchNorm2d-88          [-1, 512, 32, 32]           1,024\n",
      "           Conv2d-89          [-1, 512, 32, 32]         131,072\n",
      "           Conv2d-90          [-1, 512, 32, 32]         131,072\n",
      "      BatchNorm2d-91          [-1, 512, 32, 32]           1,024\n",
      "      BatchNorm2d-92          [-1, 512, 32, 32]           1,024\n",
      "             ReLU-93          [-1, 512, 32, 32]               0\n",
      "             ReLU-94          [-1, 512, 32, 32]               0\n",
      "       Bottleneck-95          [-1, 512, 32, 32]               0\n",
      "       Bottleneck-96          [-1, 512, 32, 32]               0\n",
      "           Conv2d-97          [-1, 128, 32, 32]          65,536\n",
      "           Conv2d-98          [-1, 128, 32, 32]          65,536\n",
      "      BatchNorm2d-99          [-1, 128, 32, 32]             256\n",
      "     BatchNorm2d-100          [-1, 128, 32, 32]             256\n",
      "            ReLU-101          [-1, 128, 32, 32]               0\n",
      "            ReLU-102          [-1, 128, 32, 32]               0\n",
      "          Conv2d-103          [-1, 128, 32, 32]         147,456\n",
      "          Conv2d-104          [-1, 128, 32, 32]         147,456\n",
      "     BatchNorm2d-105          [-1, 128, 32, 32]             256\n",
      "     BatchNorm2d-106          [-1, 128, 32, 32]             256\n",
      "            ReLU-107          [-1, 128, 32, 32]               0\n",
      "            ReLU-108          [-1, 128, 32, 32]               0\n",
      "          Conv2d-109          [-1, 512, 32, 32]          65,536\n",
      "          Conv2d-110          [-1, 512, 32, 32]          65,536\n",
      "     BatchNorm2d-111          [-1, 512, 32, 32]           1,024\n",
      "     BatchNorm2d-112          [-1, 512, 32, 32]           1,024\n",
      "            ReLU-113          [-1, 512, 32, 32]               0\n",
      "            ReLU-114          [-1, 512, 32, 32]               0\n",
      "      Bottleneck-115          [-1, 512, 32, 32]               0\n",
      "      Bottleneck-116          [-1, 512, 32, 32]               0\n",
      "          Conv2d-117          [-1, 128, 32, 32]          65,536\n",
      "          Conv2d-118          [-1, 128, 32, 32]          65,536\n",
      "     BatchNorm2d-119          [-1, 128, 32, 32]             256\n",
      "     BatchNorm2d-120          [-1, 128, 32, 32]             256\n",
      "            ReLU-121          [-1, 128, 32, 32]               0\n",
      "            ReLU-122          [-1, 128, 32, 32]               0\n",
      "          Conv2d-123          [-1, 128, 32, 32]         147,456\n",
      "          Conv2d-124          [-1, 128, 32, 32]         147,456\n",
      "     BatchNorm2d-125          [-1, 128, 32, 32]             256\n",
      "     BatchNorm2d-126          [-1, 128, 32, 32]             256\n",
      "            ReLU-127          [-1, 128, 32, 32]               0\n",
      "            ReLU-128          [-1, 128, 32, 32]               0\n",
      "          Conv2d-129          [-1, 512, 32, 32]          65,536\n",
      "          Conv2d-130          [-1, 512, 32, 32]          65,536\n",
      "     BatchNorm2d-131          [-1, 512, 32, 32]           1,024\n",
      "     BatchNorm2d-132          [-1, 512, 32, 32]           1,024\n",
      "            ReLU-133          [-1, 512, 32, 32]               0\n",
      "            ReLU-134          [-1, 512, 32, 32]               0\n",
      "      Bottleneck-135          [-1, 512, 32, 32]               0\n",
      "      Bottleneck-136          [-1, 512, 32, 32]               0\n",
      "          Conv2d-137          [-1, 128, 32, 32]          65,536\n",
      "          Conv2d-138          [-1, 128, 32, 32]          65,536\n",
      "     BatchNorm2d-139          [-1, 128, 32, 32]             256\n",
      "     BatchNorm2d-140          [-1, 128, 32, 32]             256\n",
      "            ReLU-141          [-1, 128, 32, 32]               0\n",
      "            ReLU-142          [-1, 128, 32, 32]               0\n",
      "          Conv2d-143          [-1, 128, 32, 32]         147,456\n",
      "          Conv2d-144          [-1, 128, 32, 32]         147,456\n",
      "     BatchNorm2d-145          [-1, 128, 32, 32]             256\n",
      "     BatchNorm2d-146          [-1, 128, 32, 32]             256\n",
      "            ReLU-147          [-1, 128, 32, 32]               0\n",
      "            ReLU-148          [-1, 128, 32, 32]               0\n",
      "          Conv2d-149          [-1, 512, 32, 32]          65,536\n",
      "          Conv2d-150          [-1, 512, 32, 32]          65,536\n",
      "     BatchNorm2d-151          [-1, 512, 32, 32]           1,024\n",
      "     BatchNorm2d-152          [-1, 512, 32, 32]           1,024\n",
      "            ReLU-153          [-1, 512, 32, 32]               0\n",
      "            ReLU-154          [-1, 512, 32, 32]               0\n",
      "      Bottleneck-155          [-1, 512, 32, 32]               0\n",
      "      Bottleneck-156          [-1, 512, 32, 32]               0\n",
      "          Conv2d-157          [-1, 256, 32, 32]         131,072\n",
      "          Conv2d-158          [-1, 256, 32, 32]         131,072\n",
      "     BatchNorm2d-159          [-1, 256, 32, 32]             512\n",
      "     BatchNorm2d-160          [-1, 256, 32, 32]             512\n",
      "            ReLU-161          [-1, 256, 32, 32]               0\n",
      "            ReLU-162          [-1, 256, 32, 32]               0\n",
      "          Conv2d-163          [-1, 256, 16, 16]         589,824\n",
      "          Conv2d-164          [-1, 256, 16, 16]         589,824\n",
      "     BatchNorm2d-165          [-1, 256, 16, 16]             512\n",
      "     BatchNorm2d-166          [-1, 256, 16, 16]             512\n",
      "            ReLU-167          [-1, 256, 16, 16]               0\n",
      "            ReLU-168          [-1, 256, 16, 16]               0\n",
      "          Conv2d-169         [-1, 1024, 16, 16]         262,144\n",
      "          Conv2d-170         [-1, 1024, 16, 16]         262,144\n",
      "     BatchNorm2d-171         [-1, 1024, 16, 16]           2,048\n",
      "     BatchNorm2d-172         [-1, 1024, 16, 16]           2,048\n",
      "          Conv2d-173         [-1, 1024, 16, 16]         524,288\n",
      "          Conv2d-174         [-1, 1024, 16, 16]         524,288\n",
      "     BatchNorm2d-175         [-1, 1024, 16, 16]           2,048\n",
      "     BatchNorm2d-176         [-1, 1024, 16, 16]           2,048\n",
      "            ReLU-177         [-1, 1024, 16, 16]               0\n",
      "            ReLU-178         [-1, 1024, 16, 16]               0\n",
      "      Bottleneck-179         [-1, 1024, 16, 16]               0\n",
      "      Bottleneck-180         [-1, 1024, 16, 16]               0\n",
      "          Conv2d-181          [-1, 256, 16, 16]         262,144\n",
      "          Conv2d-182          [-1, 256, 16, 16]         262,144\n",
      "     BatchNorm2d-183          [-1, 256, 16, 16]             512\n",
      "     BatchNorm2d-184          [-1, 256, 16, 16]             512\n",
      "            ReLU-185          [-1, 256, 16, 16]               0\n",
      "            ReLU-186          [-1, 256, 16, 16]               0\n",
      "          Conv2d-187          [-1, 256, 16, 16]         589,824\n",
      "          Conv2d-188          [-1, 256, 16, 16]         589,824\n",
      "     BatchNorm2d-189          [-1, 256, 16, 16]             512\n",
      "     BatchNorm2d-190          [-1, 256, 16, 16]             512\n",
      "            ReLU-191          [-1, 256, 16, 16]               0\n",
      "            ReLU-192          [-1, 256, 16, 16]               0\n",
      "          Conv2d-193         [-1, 1024, 16, 16]         262,144\n",
      "          Conv2d-194         [-1, 1024, 16, 16]         262,144\n",
      "     BatchNorm2d-195         [-1, 1024, 16, 16]           2,048\n",
      "     BatchNorm2d-196         [-1, 1024, 16, 16]           2,048\n",
      "            ReLU-197         [-1, 1024, 16, 16]               0\n",
      "            ReLU-198         [-1, 1024, 16, 16]               0\n",
      "      Bottleneck-199         [-1, 1024, 16, 16]               0\n",
      "      Bottleneck-200         [-1, 1024, 16, 16]               0\n",
      "          Conv2d-201          [-1, 256, 16, 16]         262,144\n",
      "          Conv2d-202          [-1, 256, 16, 16]         262,144\n",
      "     BatchNorm2d-203          [-1, 256, 16, 16]             512\n",
      "     BatchNorm2d-204          [-1, 256, 16, 16]             512\n",
      "            ReLU-205          [-1, 256, 16, 16]               0\n",
      "            ReLU-206          [-1, 256, 16, 16]               0\n",
      "          Conv2d-207          [-1, 256, 16, 16]         589,824\n",
      "          Conv2d-208          [-1, 256, 16, 16]         589,824\n",
      "     BatchNorm2d-209          [-1, 256, 16, 16]             512\n",
      "     BatchNorm2d-210          [-1, 256, 16, 16]             512\n",
      "            ReLU-211          [-1, 256, 16, 16]               0\n",
      "            ReLU-212          [-1, 256, 16, 16]               0\n",
      "          Conv2d-213         [-1, 1024, 16, 16]         262,144\n",
      "          Conv2d-214         [-1, 1024, 16, 16]         262,144\n",
      "     BatchNorm2d-215         [-1, 1024, 16, 16]           2,048\n",
      "     BatchNorm2d-216         [-1, 1024, 16, 16]           2,048\n",
      "            ReLU-217         [-1, 1024, 16, 16]               0\n",
      "            ReLU-218         [-1, 1024, 16, 16]               0\n",
      "      Bottleneck-219         [-1, 1024, 16, 16]               0\n",
      "      Bottleneck-220         [-1, 1024, 16, 16]               0\n",
      "          Conv2d-221          [-1, 256, 16, 16]         262,144\n",
      "          Conv2d-222          [-1, 256, 16, 16]         262,144\n",
      "     BatchNorm2d-223          [-1, 256, 16, 16]             512\n",
      "     BatchNorm2d-224          [-1, 256, 16, 16]             512\n",
      "            ReLU-225          [-1, 256, 16, 16]               0\n",
      "            ReLU-226          [-1, 256, 16, 16]               0\n",
      "          Conv2d-227          [-1, 256, 16, 16]         589,824\n",
      "          Conv2d-228          [-1, 256, 16, 16]         589,824\n",
      "     BatchNorm2d-229          [-1, 256, 16, 16]             512\n",
      "     BatchNorm2d-230          [-1, 256, 16, 16]             512\n",
      "            ReLU-231          [-1, 256, 16, 16]               0\n",
      "            ReLU-232          [-1, 256, 16, 16]               0\n",
      "          Conv2d-233         [-1, 1024, 16, 16]         262,144\n",
      "          Conv2d-234         [-1, 1024, 16, 16]         262,144\n",
      "     BatchNorm2d-235         [-1, 1024, 16, 16]           2,048\n",
      "     BatchNorm2d-236         [-1, 1024, 16, 16]           2,048\n",
      "            ReLU-237         [-1, 1024, 16, 16]               0\n",
      "            ReLU-238         [-1, 1024, 16, 16]               0\n",
      "      Bottleneck-239         [-1, 1024, 16, 16]               0\n",
      "      Bottleneck-240         [-1, 1024, 16, 16]               0\n",
      "          Conv2d-241          [-1, 256, 16, 16]         262,144\n",
      "          Conv2d-242          [-1, 256, 16, 16]         262,144\n",
      "     BatchNorm2d-243          [-1, 256, 16, 16]             512\n",
      "     BatchNorm2d-244          [-1, 256, 16, 16]             512\n",
      "            ReLU-245          [-1, 256, 16, 16]               0\n",
      "            ReLU-246          [-1, 256, 16, 16]               0\n",
      "          Conv2d-247          [-1, 256, 16, 16]         589,824\n",
      "          Conv2d-248          [-1, 256, 16, 16]         589,824\n",
      "     BatchNorm2d-249          [-1, 256, 16, 16]             512\n",
      "     BatchNorm2d-250          [-1, 256, 16, 16]             512\n",
      "            ReLU-251          [-1, 256, 16, 16]               0\n",
      "            ReLU-252          [-1, 256, 16, 16]               0\n",
      "          Conv2d-253         [-1, 1024, 16, 16]         262,144\n",
      "          Conv2d-254         [-1, 1024, 16, 16]         262,144\n",
      "     BatchNorm2d-255         [-1, 1024, 16, 16]           2,048\n",
      "     BatchNorm2d-256         [-1, 1024, 16, 16]           2,048\n",
      "            ReLU-257         [-1, 1024, 16, 16]               0\n",
      "            ReLU-258         [-1, 1024, 16, 16]               0\n",
      "      Bottleneck-259         [-1, 1024, 16, 16]               0\n",
      "      Bottleneck-260         [-1, 1024, 16, 16]               0\n",
      "          Conv2d-261          [-1, 256, 16, 16]         262,144\n",
      "          Conv2d-262          [-1, 256, 16, 16]         262,144\n",
      "     BatchNorm2d-263          [-1, 256, 16, 16]             512\n",
      "     BatchNorm2d-264          [-1, 256, 16, 16]             512\n",
      "            ReLU-265          [-1, 256, 16, 16]               0\n",
      "            ReLU-266          [-1, 256, 16, 16]               0\n",
      "          Conv2d-267          [-1, 256, 16, 16]         589,824\n",
      "          Conv2d-268          [-1, 256, 16, 16]         589,824\n",
      "     BatchNorm2d-269          [-1, 256, 16, 16]             512\n",
      "     BatchNorm2d-270          [-1, 256, 16, 16]             512\n",
      "            ReLU-271          [-1, 256, 16, 16]               0\n",
      "            ReLU-272          [-1, 256, 16, 16]               0\n",
      "          Conv2d-273         [-1, 1024, 16, 16]         262,144\n",
      "          Conv2d-274         [-1, 1024, 16, 16]         262,144\n",
      "     BatchNorm2d-275         [-1, 1024, 16, 16]           2,048\n",
      "     BatchNorm2d-276         [-1, 1024, 16, 16]           2,048\n",
      "            ReLU-277         [-1, 1024, 16, 16]               0\n",
      "            ReLU-278         [-1, 1024, 16, 16]               0\n",
      "      Bottleneck-279         [-1, 1024, 16, 16]               0\n",
      "      Bottleneck-280         [-1, 1024, 16, 16]               0\n",
      "          Conv2d-281          [-1, 512, 16, 16]         524,288\n",
      "          Conv2d-282          [-1, 512, 16, 16]         524,288\n",
      "     BatchNorm2d-283          [-1, 512, 16, 16]           1,024\n",
      "     BatchNorm2d-284          [-1, 512, 16, 16]           1,024\n",
      "            ReLU-285          [-1, 512, 16, 16]               0\n",
      "            ReLU-286          [-1, 512, 16, 16]               0\n",
      "          Conv2d-287            [-1, 512, 8, 8]       2,359,296\n",
      "          Conv2d-288            [-1, 512, 8, 8]       2,359,296\n",
      "     BatchNorm2d-289            [-1, 512, 8, 8]           1,024\n",
      "     BatchNorm2d-290            [-1, 512, 8, 8]           1,024\n",
      "            ReLU-291            [-1, 512, 8, 8]               0\n",
      "            ReLU-292            [-1, 512, 8, 8]               0\n",
      "          Conv2d-293           [-1, 2048, 8, 8]       1,048,576\n",
      "          Conv2d-294           [-1, 2048, 8, 8]       1,048,576\n",
      "     BatchNorm2d-295           [-1, 2048, 8, 8]           4,096\n",
      "     BatchNorm2d-296           [-1, 2048, 8, 8]           4,096\n",
      "          Conv2d-297           [-1, 2048, 8, 8]       2,097,152\n",
      "          Conv2d-298           [-1, 2048, 8, 8]       2,097,152\n",
      "     BatchNorm2d-299           [-1, 2048, 8, 8]           4,096\n",
      "     BatchNorm2d-300           [-1, 2048, 8, 8]           4,096\n",
      "            ReLU-301           [-1, 2048, 8, 8]               0\n",
      "            ReLU-302           [-1, 2048, 8, 8]               0\n",
      "      Bottleneck-303           [-1, 2048, 8, 8]               0\n",
      "      Bottleneck-304           [-1, 2048, 8, 8]               0\n",
      "          Conv2d-305            [-1, 512, 8, 8]       1,048,576\n",
      "          Conv2d-306            [-1, 512, 8, 8]       1,048,576\n",
      "     BatchNorm2d-307            [-1, 512, 8, 8]           1,024\n",
      "     BatchNorm2d-308            [-1, 512, 8, 8]           1,024\n",
      "            ReLU-309            [-1, 512, 8, 8]               0\n",
      "            ReLU-310            [-1, 512, 8, 8]               0\n",
      "          Conv2d-311            [-1, 512, 8, 8]       2,359,296\n",
      "          Conv2d-312            [-1, 512, 8, 8]       2,359,296\n",
      "     BatchNorm2d-313            [-1, 512, 8, 8]           1,024\n",
      "     BatchNorm2d-314            [-1, 512, 8, 8]           1,024\n",
      "            ReLU-315            [-1, 512, 8, 8]               0\n",
      "            ReLU-316            [-1, 512, 8, 8]               0\n",
      "          Conv2d-317           [-1, 2048, 8, 8]       1,048,576\n",
      "          Conv2d-318           [-1, 2048, 8, 8]       1,048,576\n",
      "     BatchNorm2d-319           [-1, 2048, 8, 8]           4,096\n",
      "     BatchNorm2d-320           [-1, 2048, 8, 8]           4,096\n",
      "            ReLU-321           [-1, 2048, 8, 8]               0\n",
      "            ReLU-322           [-1, 2048, 8, 8]               0\n",
      "      Bottleneck-323           [-1, 2048, 8, 8]               0\n",
      "      Bottleneck-324           [-1, 2048, 8, 8]               0\n",
      "          Conv2d-325            [-1, 512, 8, 8]       1,048,576\n",
      "          Conv2d-326            [-1, 512, 8, 8]       1,048,576\n",
      "     BatchNorm2d-327            [-1, 512, 8, 8]           1,024\n",
      "     BatchNorm2d-328            [-1, 512, 8, 8]           1,024\n",
      "            ReLU-329            [-1, 512, 8, 8]               0\n",
      "            ReLU-330            [-1, 512, 8, 8]               0\n",
      "          Conv2d-331            [-1, 512, 8, 8]       2,359,296\n",
      "          Conv2d-332            [-1, 512, 8, 8]       2,359,296\n",
      "     BatchNorm2d-333            [-1, 512, 8, 8]           1,024\n",
      "     BatchNorm2d-334            [-1, 512, 8, 8]           1,024\n",
      "            ReLU-335            [-1, 512, 8, 8]               0\n",
      "            ReLU-336            [-1, 512, 8, 8]               0\n",
      "          Conv2d-337           [-1, 2048, 8, 8]       1,048,576\n",
      "          Conv2d-338           [-1, 2048, 8, 8]       1,048,576\n",
      "     BatchNorm2d-339           [-1, 2048, 8, 8]           4,096\n",
      "     BatchNorm2d-340           [-1, 2048, 8, 8]           4,096\n",
      "            ReLU-341           [-1, 2048, 8, 8]               0\n",
      "            ReLU-342           [-1, 2048, 8, 8]               0\n",
      "      Bottleneck-343           [-1, 2048, 8, 8]               0\n",
      "      Bottleneck-344           [-1, 2048, 8, 8]               0\n",
      "          Conv2d-345            [-1, 256, 1, 1]         524,544\n",
      "          Conv2d-346            [-1, 256, 8, 8]         524,544\n",
      "          Conv2d-347          [-1, 256, 16, 16]         262,400\n",
      "          Conv2d-348          [-1, 256, 32, 32]         131,328\n",
      "          Conv2d-349          [-1, 256, 64, 64]          65,792\n",
      "          Conv2d-350            [-1, 256, 8, 8]          65,792\n",
      "     BatchNorm2d-351            [-1, 256, 8, 8]             512\n",
      "            ReLU-352            [-1, 256, 8, 8]               0\n",
      "        Conv2dBN-353            [-1, 256, 8, 8]               0\n",
      "          Conv2d-354            [-1, 256, 8, 8]          65,792\n",
      "     BatchNorm2d-355            [-1, 256, 8, 8]             512\n",
      "            ReLU-356            [-1, 256, 8, 8]               0\n",
      "        Conv2dBN-357            [-1, 256, 8, 8]               0\n",
      "        FSModule-358            [-1, 256, 8, 8]               0\n",
      "          Conv2d-359          [-1, 256, 16, 16]          65,792\n",
      "     BatchNorm2d-360          [-1, 256, 16, 16]             512\n",
      "            ReLU-361          [-1, 256, 16, 16]               0\n",
      "        Conv2dBN-362          [-1, 256, 16, 16]               0\n",
      "          Conv2d-363          [-1, 256, 16, 16]          65,792\n",
      "     BatchNorm2d-364          [-1, 256, 16, 16]             512\n",
      "            ReLU-365          [-1, 256, 16, 16]               0\n",
      "        Conv2dBN-366          [-1, 256, 16, 16]               0\n",
      "        FSModule-367          [-1, 256, 16, 16]               0\n",
      "          Conv2d-368          [-1, 256, 32, 32]          65,792\n",
      "     BatchNorm2d-369          [-1, 256, 32, 32]             512\n",
      "            ReLU-370          [-1, 256, 32, 32]               0\n",
      "        Conv2dBN-371          [-1, 256, 32, 32]               0\n",
      "          Conv2d-372          [-1, 256, 32, 32]          65,792\n",
      "     BatchNorm2d-373          [-1, 256, 32, 32]             512\n",
      "            ReLU-374          [-1, 256, 32, 32]               0\n",
      "        Conv2dBN-375          [-1, 256, 32, 32]               0\n",
      "        FSModule-376          [-1, 256, 32, 32]               0\n",
      "          Conv2d-377          [-1, 256, 64, 64]          65,792\n",
      "     BatchNorm2d-378          [-1, 256, 64, 64]             512\n",
      "            ReLU-379          [-1, 256, 64, 64]               0\n",
      "        Conv2dBN-380          [-1, 256, 64, 64]               0\n",
      "          Conv2d-381          [-1, 256, 64, 64]          65,792\n",
      "     BatchNorm2d-382          [-1, 256, 64, 64]             512\n",
      "            ReLU-383          [-1, 256, 64, 64]               0\n",
      "        Conv2dBN-384          [-1, 256, 64, 64]               0\n",
      "        FSModule-385          [-1, 256, 64, 64]               0\n",
      "          Conv2d-386            [-1, 256, 8, 8]         590,080\n",
      "     BatchNorm2d-387            [-1, 256, 8, 8]             512\n",
      "            ReLU-388            [-1, 256, 8, 8]               0\n",
      "        Conv2dBN-389            [-1, 256, 8, 8]               0\n",
      "          Conv2d-390          [-1, 256, 16, 16]         590,080\n",
      "     BatchNorm2d-391          [-1, 256, 16, 16]             512\n",
      "            ReLU-392          [-1, 256, 16, 16]               0\n",
      "        Conv2dBN-393          [-1, 256, 16, 16]               0\n",
      "          Conv2d-394          [-1, 256, 32, 32]         590,080\n",
      "     BatchNorm2d-395          [-1, 256, 32, 32]             512\n",
      "            ReLU-396          [-1, 256, 32, 32]               0\n",
      "        Conv2dBN-397          [-1, 256, 32, 32]               0\n",
      "         Decoder-398          [-1, 256, 64, 64]               0\n",
      "          Conv2d-399          [-1, 256, 16, 16]         590,080\n",
      "     BatchNorm2d-400          [-1, 256, 16, 16]             512\n",
      "            ReLU-401          [-1, 256, 16, 16]               0\n",
      "        Conv2dBN-402          [-1, 256, 16, 16]               0\n",
      "          Conv2d-403          [-1, 256, 32, 32]         590,080\n",
      "     BatchNorm2d-404          [-1, 256, 32, 32]             512\n",
      "            ReLU-405          [-1, 256, 32, 32]               0\n",
      "        Conv2dBN-406          [-1, 256, 32, 32]               0\n",
      "         Decoder-407          [-1, 256, 64, 64]               0\n",
      "          Conv2d-408          [-1, 256, 32, 32]         590,080\n",
      "     BatchNorm2d-409          [-1, 256, 32, 32]             512\n",
      "            ReLU-410          [-1, 256, 32, 32]               0\n",
      "        Conv2dBN-411          [-1, 256, 32, 32]               0\n",
      "         Decoder-412          [-1, 256, 64, 64]               0\n",
      "          Conv2d-413          [-1, 256, 64, 64]         590,080\n",
      "     BatchNorm2d-414          [-1, 256, 64, 64]             512\n",
      "            ReLU-415          [-1, 256, 64, 64]               0\n",
      "        Conv2dBN-416          [-1, 256, 64, 64]               0\n",
      "         Decoder-417          [-1, 256, 64, 64]               0\n",
      "          Conv2d-418          [-1, 1, 256, 256]           2,305\n",
      "================================================================\n",
      "Total params: 53,191,553\n",
      "Trainable params: 53,191,553\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.75\n",
      "Forward/backward pass size (MB): 947.75\n",
      "Params size (MB): 202.91\n",
      "Estimated Total Size (MB): 1151.41\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(net, (3, 256, 256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

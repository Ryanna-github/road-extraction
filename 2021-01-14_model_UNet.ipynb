{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torch.autograd import Function\n",
    "from torchsummary import summary\n",
    "\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "# from torch.utils.data import , random_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# U-Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCH_NUM = 50\n",
    "BATCH_SIZE = 4\n",
    "LR = 0.0005\n",
    "# INPUT_SIZE, OUTPUT_SIZE = 512, 324\n",
    "INPUT_SIZE, OUTPUT_SIZE = 256, 68\n",
    "\n",
    "# root_path = 'D://Data/massachusetts-roads-dataset/'\n",
    "root_path = '/home/renyan/ossdata/massachusetts-roads-dataset/'\n",
    "road_path = root_path + \"tiff_select2_parts_16/\"\n",
    "\n",
    "DIR_CHECKPOINT = 'checkpoints/'\n",
    "\n",
    "classes = ['background', 'road']\n",
    "colormap = [[0 , 0, 0], [255, 255, 255]]\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleConv(nn.Module):\n",
    "    # 输入输出的 feature map 个数可以控制\n",
    "    # 但是图片具体大小根据图片本身大小决定\n",
    "    def __init__(self, in_channels, out_channels, mid_channels=None):\n",
    "        super().__init__()\n",
    "        if not mid_channels:\n",
    "            mid_channels = out_channels\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=0),\n",
    "            nn.BatchNorm2d(mid_channels),\n",
    "            nn.ReLU(inplace = True),\n",
    "            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=0),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace = True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Down(nn.Module):\n",
    "    \"\"\"Downscaling with maxpool then double conv\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.maxpool_conv = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            DoubleConv(in_channels, out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.maxpool_conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Up(nn.Module):\n",
    "    \"\"\"Upscaling then double conv\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, bilinear=True):\n",
    "        super().__init__()\n",
    "\n",
    "        # if bilinear, use the normal convolutions to reduce the number of channels\n",
    "        if bilinear:\n",
    "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "            self.conv = DoubleConv(in_channels, out_channels, in_channels // 2)\n",
    "        else:\n",
    "            self.up = nn.ConvTranspose2d(in_channels , in_channels // 2, kernel_size=2, stride=2)\n",
    "            self.conv = DoubleConv(in_channels, out_channels)\n",
    "\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "        diffY = x2.size()[2] - x1.size()[2]\n",
    "        diffX = x2.size()[3] - x1.size()[3]\n",
    "\n",
    "#         x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
    "#                         diffY // 2, diffY - diffY // 2])\n",
    "        x2 = x2[:,:, (diffY // 2): -(diffY - diffY // 2), (diffY // 2): -(diffX - diffX // 2)]\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        return self.conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OutConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(OutConv, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self, n_channels, n_classes, bilinear=True):\n",
    "        super(UNet, self).__init__()\n",
    "        self.n_channels = n_channels # 图片的通道数（最初输入）\n",
    "        self.n_classes = n_classes # 最终类别墅（最终输出）\n",
    "        self.bilinear = bilinear # -- 暂不用\n",
    "\n",
    "        self.inc = DoubleConv(n_channels, 64)\n",
    "        self.down1 = Down(64, 128)\n",
    "        self.down2 = Down(128, 256)\n",
    "        self.down3 = Down(256, 512)\n",
    "        factor = 2 if bilinear else 1\n",
    "        self.down4 = Down(512, 1024 // factor)\n",
    "        self.up1 = Up(1024, 512 // factor, bilinear)\n",
    "        self.up2 = Up(512, 256 // factor, bilinear)\n",
    "        self.up3 = Up(256, 128 // factor, bilinear)\n",
    "        self.up4 = Up(128, 64, bilinear)\n",
    "        self.outc = OutConv(64, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "        x = self.up1(x5, x4)\n",
    "        x = self.up2(x, x3)\n",
    "        x = self.up3(x, x2)\n",
    "        x = self.up4(x, x1)\n",
    "        logits = self.outc(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tnet = UNet(3, 1, False).to(device)\n",
    "# summary(tnet, (3, 256, 256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/renyan/ossdata/massachusetts-roads-dataset/tiff_select2_parts_16/'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "road_path "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((375, 375, 3), (375, 375, 3))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# img = cv2.imread(road_path + 'train/11128720_15_p1.tiff')\n",
    "# label = cv2.imread(road_path + 'train_labels/11128720_15_p1.tif')\n",
    "img = cv2.imread(road_path + 'train/10228705_15_h1_w1.tiff')\n",
    "label = cv2.imread(road_path + 'train_labels/10228705_15_h1_w1.tif')\n",
    "img.shape, label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform = transforms.Compose([transforms.Resize([512, 512], 0),\n",
    "#                                 transforms.ToTensor(),])\n",
    "# img_tensor, label_tensor = transform(Image.fromarray(img)), transform(Image.fromarray(label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(img_tensor.permute(1, 2, 0).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(label_tensor.permute(1, 2, 0).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Dice coefficient: \n",
    "    - measure the similarity between two set.\n",
    "    - $\\frac{2|X\\cap Y|}{|X| + |Y|}$\n",
    "- Dice loss: $1 - \\frac{2|X\\cap Y|}{|X| + |Y|}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiceCoeff(Function):\n",
    "    \"\"\"Dice coeff for individual examples\"\"\"\n",
    "\n",
    "    # 在进入 forward 之前，所有变量都会被转化为 tensor\n",
    "    def forward(self, input, target):\n",
    "        self.save_for_backward(input, target) # tensor 转化为变量保存到后续操作\n",
    "        eps = 0.0001\n",
    "        self.inter = torch.dot(input.view(-1), target.view(-1))\n",
    "        self.union = torch.sum(input) + torch.sum(target) + eps\n",
    "\n",
    "        t = (2 * self.inter.float() + eps) / self.union.float()\n",
    "        return t\n",
    "\n",
    "    # This function has only a single output, so it gets only one gradient\n",
    "    def backward(self, grad_output):\n",
    "        input, target = self.saved_variables\n",
    "        grad_input = grad_target = None\n",
    "\n",
    "        # 判断 input 是否需要求梯度\n",
    "        if self.needs_input_grad[0]:\n",
    "            grad_input = grad_output * 2 * (target * self.union - self.inter) \\\n",
    "                         / (self.union * self.union)\n",
    "        # 判断 target 是否需要求梯度\n",
    "        if self.needs_input_grad[1]:\n",
    "            grad_target = None\n",
    "\n",
    "        return grad_input, grad_target\n",
    "\n",
    "\n",
    "def dice_coeff(input, target):\n",
    "    \"\"\"Dice coeff for batches\"\"\"\n",
    "    # 在合适的设备上初始化一个1*1零向量\n",
    "    # 同一个 batch 中 dice loss 取平均\n",
    "    s = torch.FloatTensor(1).cuda().zero_() if input.is_cuda else torch.FloatTensor(1).zero_()\n",
    "    for i, c in enumerate(zip(input, target)):\n",
    "        s = s + DiceCoeff().forward(c[0], c[1])\n",
    "    return s / (i + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_net(net, loader, device):\n",
    "    \"\"\"Evaluation without the densecrf with the dice coefficient\"\"\"\n",
    "    # 关闭 batchnorm 和 dropout\n",
    "    net.eval() # 仔细看\n",
    "    mask_type = torch.float32 if net.n_classes == 1 else torch.long\n",
    "    n_val = len(loader)  # the number of batch\n",
    "    tot = 0\n",
    "\n",
    "    # 括号里设置文字输出信息\n",
    "#     with tqdm(total = n_val, desc='Validation round', unit='batch', leave = False) as pbar:\n",
    "        # 对于每个 batch\n",
    "    for batch in loader:\n",
    "        imgs, true_masks = batch[0], batch[1]\n",
    "        imgs = imgs.to(device=device, dtype=torch.float32)\n",
    "        true_masks = true_masks.to(device=device, dtype=mask_type)\n",
    "\n",
    "        # 不需要追踪梯度变化，不需要进行反向传播，提升速度\n",
    "        with torch.no_grad():\n",
    "            # 得到模型预测结果\n",
    "            mask_pred = net(imgs)\n",
    "\n",
    "        # 不同类别预测结果损失累加\n",
    "        if net.n_classes > 1:\n",
    "            tot += F.cross_entropy(mask_pred, true_masks).item()\n",
    "        else:\n",
    "            pred = torch.sigmoid(mask_pred)\n",
    "            pred = (pred > 0.5).float()\n",
    "            tot += dice_coeff(pred, true_masks).item()\n",
    "#             pbar.update()\n",
    "\n",
    "    net.train()\n",
    "    return tot / n_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取图片及其标签路径（图片和标签配对）\n",
    "def read_images(root_path, train):\n",
    "    type_str = \"train\" if train else \"val\"\n",
    "    data = [root_path + \"/\" + type_str + \"/\" + file_name for file_name in list(os.walk(root_path+\"/\" + type_str + \"/\"))[0][2]]\n",
    "    file_names_order = [path.split(\"/\")[-1][:-1] for path in data]\n",
    "    labels = [root_path + \"/\" + type_str + \"_labels/\" + file_name for file_name in file_names_order]\n",
    "    return data, labels\n",
    "\n",
    "# 原图 0-255，tensor 中 0-1\n",
    "def change_to_tensor(img, label, tensor_size, output_size):\n",
    "    img = cv2.imread(img)\n",
    "    label = cv2.imread(label)\n",
    "    # 对于每一种类别\n",
    "    for i, cm in enumerate(colormap):\n",
    "        # 每个通道都符合条件（在本例子中实际只限制一个通道即可）\n",
    "        label[(label[:, :, 0] == cm[0]) & (label[:, :, 1] == cm[1]) & (label[:, :, 2] == cm[2])] = i * 255\n",
    "    label = label[:, :, 0] # 只取一个通道结果作为标签\n",
    "    \n",
    "    transform_data = transforms.Compose([transforms.Resize([tensor_size, tensor_size], 0),\n",
    "                                    transforms.ToTensor(),])\n",
    "    transform_label = transforms.Compose([transforms.Resize([output_size, output_size], 0),\n",
    "                                    transforms.ToTensor(),])\n",
    "    img_tensor, label_tensor = transform_data(Image.fromarray(img)), transform_label(Image.fromarray(label))\n",
    "    return img_tensor, label_tensor\n",
    "\n",
    "class RoadDataset(Dataset):\n",
    "    def __init__(self, root_path, train, input_size, output_size):\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        super(RoadDataset, self).__init__()\n",
    "        \n",
    "        self.data_list, self.label_list = read_images(root_path = root_path, train = train)\n",
    "        self.len = len(self.data_list)\n",
    "        print('Read '+ str(self.len)+' images')\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img = self.data_list[idx]\n",
    "        label = self.label_list[idx]\n",
    "        img, label = change_to_tensor(img, label, self.input_size, self.output_size)\n",
    "        return img, label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if ground truth is paired with corresponding data\n",
    "# img_list, lbl_list = read_images(road_path)\n",
    "# ll = []\n",
    "# for i, (img, lbl) in enumerate(zip(img_list, lbl_list)):\n",
    "#     ll.append(img.split(\"/\")[-1].split(\".\")[0] == lbl.split(\"/\")[-1].split(\".\")[0])\n",
    "# print(sum(ll), len(ll))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check train()\n",
    "# net = UNet(n_channels = 3, n_classes = 1, bilinear = False).to(device)\n",
    "# train_dataset = RoadDataset(road_path, True, INPUT_SIZE, OUTPUT_SIZE)\n",
    "# val_dataset = RoadDataset(road_path, False, INPUT_SIZE, OUTPUT_SIZE)\n",
    "# train_loader = DataLoader(train_dataset, BATCH_SIZE, shuffle = False)\n",
    "# val_loader = DataLoader(val_dataset, batch_size = BATCH_SIZE, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr = 0.0001\n",
    "# n_val = len(val_dataset)\n",
    "# n_train = len(train_dataset)\n",
    "# writer = SummaryWriter(comment=f'LR_{lr}_BS_{BATCH_SIZE}')\n",
    "# optimizer = optim.SGD(net.parameters(), lr=lr, weight_decay=1e-8, momentum=0.9)\n",
    "# scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', patience=2)\n",
    "# criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# net.train()\n",
    "# epoch_loss = 0\n",
    "# imgs, true_masks = next(iter(train_loader))\n",
    "# imgs = imgs.to(device = device, dtype = torch.float32)\n",
    "# mask_type = torch.float32\n",
    "# true_masks = true_masks.to(device = device, dtype = mask_type)\n",
    "# masks_pred = net(imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_net(net, device, train_dataset, val_dataset, epochs = EPOCH_NUM, lr = LR, save_cp = True,\n",
    "             batch_size = BATCH_SIZE):\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, BATCH_SIZE, shuffle = False)\n",
    "    val_loader = DataLoader(val_dataset, batch_size = BATCH_SIZE, shuffle = False)\n",
    "    \n",
    "    # 每轮 evaluation 检验的 batch 个数\n",
    "    n_val = len(val_dataset)\n",
    "    # 每轮 train 检验的 batch 个数\n",
    "    n_train = len(train_dataset)\n",
    "\n",
    "    writer = SummaryWriter(comment=f'LR_{lr}_BS_{BATCH_SIZE}')\n",
    "    global_step = 0\n",
    "\n",
    "    logging.info(f'''Starting training:\n",
    "        Epochs:          {epochs}\n",
    "        Batch size:      {batch_size}\n",
    "        Learning rate:   {lr}\n",
    "        Training size:   {n_train}\n",
    "        Validation size: {n_val}\n",
    "        Checkpoints:     {save_cp}\n",
    "        Device:          {device.type}\n",
    "    ''')\n",
    "#     换 SGD，图像用 SGD Adam，收敛速度而非效果\n",
    "#     optimizer = optim.RMSprop(net.parameters(), lr=lr, weight_decay=1e-8, momentum=0.9)\n",
    "    optimizer = optim.SGD(net.parameters(), lr=lr, weight_decay=1e-8, momentum=0.9)\n",
    "    # 在发现loss不再降低或者acc不再提高之后，降低学习率。patience 含义：不再减小（或增大）的累计次数\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min' if net.n_classes > 1 else 'max', patience=2)\n",
    "    \n",
    "    if net.n_classes > 1:\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "    else:\n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        net.train()\n",
    "        print(net.inc.double_conv[0].weight[0,0,0])\n",
    "\n",
    "        epoch_loss = 0\n",
    "        with tqdm(total = n_train, desc=f'Epoch {epoch + 1}/{epochs}', unit='img') as pbar:\n",
    "            for batch in train_loader:\n",
    "                \n",
    "                imgs = batch[0]\n",
    "                true_masks = batch[1]\n",
    "                \n",
    "                assert imgs.shape[1] == net.n_channels, \\\n",
    "                    f'Network has been defined with {net.n_channels} input channels, ' \\\n",
    "                    f'but loaded images have {imgs.shape[1]} channels. Please check that ' \\\n",
    "                    'the images are loaded correctly.'\n",
    "\n",
    "                imgs = imgs.to(device = device, dtype = torch.float32)\n",
    "                mask_type = torch.float32 if net.n_classes == 1 else torch.long\n",
    "                true_masks = true_masks.to(device = device, dtype = mask_type) # 01\n",
    "\n",
    "                masks_pred = net(imgs)\n",
    "                loss = criterion(masks_pred, true_masks)\n",
    "                epoch_loss += loss.item()\n",
    "                writer.add_scalar('Loss/train', loss.item(), global_step)\n",
    "\n",
    "                pbar.set_postfix(**{'loss (batch)': loss.item()})\n",
    "\n",
    "                # 对于每个 batch 都要更新一次参数空间\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                # 防止梯度爆炸，设置梯度截断\n",
    "                nn.utils.clip_grad_value_(net.parameters(), 1)\n",
    "                optimizer.step()\n",
    "\n",
    "                # 每个 batch 结束更新一次进度条，迭代器内部计数器累加 batch 的大小\n",
    "                pbar.update(imgs.shape[0])\n",
    "                global_step += 1\n",
    "                \n",
    "                # 在 tensorboard 中记录一次\n",
    "                if global_step % (n_train // (10 * batch_size) + 1) == 0:\n",
    "                    for tag, value in net.named_parameters():\n",
    "                        tag = tag.replace('.', '/')\n",
    "                        writer.add_histogram('weights/' + tag, value.data.cpu().numpy(), global_step)\n",
    "                        writer.add_histogram('grads/' + tag, value.grad.data.cpu().numpy(), global_step)\n",
    "                    val_score = eval_net(net, val_loader, device)\n",
    "                    scheduler.step(val_score)\n",
    "                    writer.add_scalar('learning_rate', optimizer.param_groups[0]['lr'], global_step)\n",
    "\n",
    "                    if net.n_classes > 1:\n",
    "                        logging.info('Validation cross entropy: {}'.format(val_score))\n",
    "                        writer.add_scalar('Loss/test', val_score, global_step)\n",
    "                    else:\n",
    "                        logging.info('Validation Dice Coeff: {}'.format(val_score))\n",
    "                        writer.add_scalar('Dice/test', val_score, global_step)\n",
    "\n",
    "                    writer.add_images('images', imgs, global_step)\n",
    "                    if net.n_classes == 1:\n",
    "                        writer.add_images('masks/true', true_masks, global_step)\n",
    "                        writer.add_images('masks/pred_0.5', torch.sigmoid(masks_pred) > 0.5, global_step)\n",
    "                        writer.add_images('masks/pred_0.4', torch.sigmoid(masks_pred) > 0.4, global_step)\n",
    "                        writer.add_images('masks/pred_0.3', torch.sigmoid(masks_pred) > 0.3, global_step)\n",
    "                        writer.add_images('masks/pred_0.2', torch.sigmoid(masks_pred) > 0.2, global_step)\n",
    "                        writer.add_images('masks/pred_0.1', torch.sigmoid(masks_pred) > 0.1, global_step)\n",
    "\n",
    "        if save_cp:\n",
    "            try:\n",
    "                os.mkdir(DIR_CHECKPOINT)\n",
    "                logging.info('Created checkpoint directory')\n",
    "            except OSError:\n",
    "                pass\n",
    "            torch.save(net.state_dict(),\n",
    "                       DIR_CHECKPOINT + f'unet_epoch{epoch + 1}.pth')\n",
    "            logging.info(f'Checkpoint {epoch + 1} saved.')\n",
    "            if os.path.exists(DIR_CHECKPOINT + f'unet_epoch{epoch - 4}.pth') & (epoch - 4)//10 != 0:\n",
    "                os.remove(DIR_CHECKPOINT + f'unet_epoch{epoch - 4}.pth')\n",
    "                logging.info(f'Checkpoint {epoch - 4} deleted.')\n",
    "\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# net = UNet(n_channels = 3, n_classes = 1, bilinear = False)\n",
    "# t = net(img_tensor.unsqueeze(0))\n",
    "# t = (torch.sigmoid(t).squeeze(0) > 0.5) * 255\n",
    "# t = t.detach().numpy()\n",
    "# t = np.concatenate([t, t, t])\n",
    "# t = t.transpose((1, 2, 0))\n",
    "# plt.imshow(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check\n",
    "# import pandas as pd\n",
    "# pd.DataFrame({\"img\": [t.split(\"/\")[-1] for t in train_dataset.data_list],\n",
    "#               \"lbl\": [t.split(\"/\")[-1] for t in train_dataset.label_list]}).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 7056 images\n",
      "Read 224 images\n"
     ]
    }
   ],
   "source": [
    "net = UNet(n_channels = 3, n_classes = 1, bilinear = False).to(device)\n",
    "train_dataset = RoadDataset(road_path, True, INPUT_SIZE, OUTPUT_SIZE)\n",
    "val_dataset = RoadDataset(road_path, False, INPUT_SIZE, OUTPUT_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/50:   0%|          | 0/7056 [00:00<?, ?img/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0201, -0.1721, -0.1787], device='cuda:0', grad_fn=<SelectBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50: 100%|██████████| 7056/7056 [23:26<00:00,  5.02img/s, loss (batch)=0.371]  \n",
      "Epoch 2/50:   0%|          | 0/7056 [00:00<?, ?img/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0201, -0.1721, -0.1787], device='cuda:0', grad_fn=<SelectBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/50: 100%|██████████| 7056/7056 [17:32<00:00,  6.70img/s, loss (batch)=0.371]  \n",
      "Epoch 3/50:   0%|          | 0/7056 [00:00<?, ?img/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0201, -0.1721, -0.1787], device='cuda:0', grad_fn=<SelectBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/50:   1%|          | 48/7056 [00:20<50:48,  2.30img/s, loss (batch)=0.258]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-228c6c3b4e61>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-40-69aaf61fd15f>\u001b[0m in \u001b[0;36mtrain_net\u001b[0;34m(net, device, train_dataset, val_dataset, epochs, lr, save_cp, batch_size)\u001b[0m\n\u001b[1;32m     77\u001b[0m                         \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_histogram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'weights/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m                         \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_histogram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'grads/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m                     \u001b[0mval_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m                     \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m                     \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'learning_rate'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_groups\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-34-b7b2ea6bc62f>\u001b[0m in \u001b[0;36meval_net\u001b[0;34m(net, loader, device)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m#     with tqdm(total = n_val, desc='Validation round', unit='batch', leave = False) as pbar:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;31m# 对于每个 batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mimgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrue_masks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mimgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    433\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    473\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-35-9622b6841c21>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchange_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-35-9622b6841c21>\u001b[0m in \u001b[0;36mchange_to_tensor\u001b[0;34m(img, label, tensor_size, output_size)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# 原图 0-255，tensor 中 0-1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mchange_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m# 对于每一种类别\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_net(net, device, train_dataset, val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

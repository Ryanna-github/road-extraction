{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from torch.autograd import Function\n",
    "from torchsummary import summary\n",
    "\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "from torch import optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "# from unet import UNet\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader, random_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# U-Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCH_NUM = 100\n",
    "BATCH_SIZE = 4\n",
    "LR = 0.001\n",
    "INPUT_SIZE = 512 # 输入网络的大小而非图片大小\n",
    "OUTPUT_SIZE = 324\n",
    "\n",
    "# root_path = 'D://Data/massachusetts-roads-dataset/'\n",
    "root_path = '/home/renyan/ossdata/massachusetts-roads-dataset/'\n",
    "road_path = root_path + \"tiff_select_parts/\"\n",
    "\n",
    "DIR_CHECKPOINT = 'checkpoints/'\n",
    "\n",
    "classes = ['background', 'road']\n",
    "colormap = [[0 , 0, 0], [255, 255, 255]]\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleConv(nn.Module):\n",
    "    # 输入输出的 feature map 个数可以控制\n",
    "    # 但是图片具体大小根据图片本身大小决定\n",
    "    def __init__(self, in_channels, out_channels, mid_channels=None):\n",
    "        super().__init__()\n",
    "        if not mid_channels:\n",
    "            mid_channels = out_channels\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=0),\n",
    "            nn.BatchNorm2d(mid_channels),\n",
    "            nn.ReLU(inplace = True),\n",
    "            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=0),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace = True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Down(nn.Module):\n",
    "    \"\"\"Downscaling with maxpool then double conv\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.maxpool_conv = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            DoubleConv(in_channels, out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.maxpool_conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Up(nn.Module):\n",
    "    \"\"\"Upscaling then double conv\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, bilinear=True):\n",
    "        super().__init__()\n",
    "\n",
    "        # if bilinear, use the normal convolutions to reduce the number of channels\n",
    "        if bilinear:\n",
    "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "            self.conv = DoubleConv(in_channels, out_channels, in_channels // 2)\n",
    "        else:\n",
    "            self.up = nn.ConvTranspose2d(in_channels , in_channels // 2, kernel_size=2, stride=2)\n",
    "            self.conv = DoubleConv(in_channels, out_channels)\n",
    "\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "        diffY = x2.size()[2] - x1.size()[2]\n",
    "        diffX = x2.size()[3] - x1.size()[3]\n",
    "\n",
    "#         x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
    "#                         diffY // 2, diffY - diffY // 2])\n",
    "        x2 = x2[:,:, (diffY // 2): -(diffY - diffY // 2), (diffY // 2): -(diffX - diffX // 2)]\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        return self.conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OutConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(OutConv, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self, n_channels, n_classes, bilinear=True):\n",
    "        super(UNet, self).__init__()\n",
    "        self.n_channels = n_channels # 图片的通道数（最初输入）\n",
    "        self.n_classes = n_classes # 最终类别墅（最终输出）\n",
    "        self.bilinear = bilinear # -- 暂不用\n",
    "\n",
    "        self.inc = DoubleConv(n_channels, 64)\n",
    "        self.down1 = Down(64, 128)\n",
    "        self.down2 = Down(128, 256)\n",
    "        self.down3 = Down(256, 512)\n",
    "        factor = 2 if bilinear else 1\n",
    "        self.down4 = Down(512, 1024 // factor)\n",
    "        self.up1 = Up(1024, 512 // factor, bilinear)\n",
    "        self.up2 = Up(512, 256 // factor, bilinear)\n",
    "        self.up3 = Up(256, 128 // factor, bilinear)\n",
    "        self.up4 = Up(128, 64, bilinear)\n",
    "        self.outc = OutConv(64, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "        x = self.up1(x5, x4)\n",
    "        x = self.up2(x, x3)\n",
    "        x = self.up3(x, x2)\n",
    "        x = self.up4(x, x1)\n",
    "        logits = self.outc(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tnet = UNet(3, 1, False).to(device)\n",
    "# summary(tnet, (3, 512, 512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/renyan/ossdata/massachusetts-roads-dataset/tiff_select_parts/'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "road_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((750, 750, 3), (750, 750, 3))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = cv2.imread(road_path + 'train/11128720_15_p1.tiff')\n",
    "label = cv2.imread(road_path + 'train_labels/11128720_15_p1.tif')\n",
    "img.shape, label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.Resize([512, 512], 0),\n",
    "                                transforms.ToTensor(),])\n",
    "img_tensor, label_tensor = transform(Image.fromarray(img)), transform(Image.fromarray(label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(img_tensor.permute(1, 2, 0).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(label_tensor.permute(1, 2, 0).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Dice coefficient: \n",
    "    - measure the similarity between two set.\n",
    "    - $\\frac{2|X\\cap Y|}{|X| + |Y|}$\n",
    "- Dice loss: $1 - \\frac{2|X\\cap Y|}{|X| + |Y|}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiceCoeff(Function):\n",
    "    \"\"\"Dice coeff for individual examples\"\"\"\n",
    "\n",
    "    # 在进入 forward 之前，所有变量都会被转化为 tensor\n",
    "    def forward(self, input, target):\n",
    "        self.save_for_backward(input, target) # tensor 转化为变量保存到后续操作\n",
    "        eps = 0.0001\n",
    "        self.inter = torch.dot(input.view(-1), target.view(-1))\n",
    "        self.union = torch.sum(input) + torch.sum(target) + eps\n",
    "\n",
    "        t = (2 * self.inter.float() + eps) / self.union.float()\n",
    "        return t\n",
    "\n",
    "    # This function has only a single output, so it gets only one gradient\n",
    "    def backward(self, grad_output):\n",
    "        input, target = self.saved_variables\n",
    "        grad_input = grad_target = None\n",
    "\n",
    "        # 判断 input 是否需要求梯度\n",
    "        if self.needs_input_grad[0]:\n",
    "            grad_input = grad_output * 2 * (target * self.union - self.inter) \\\n",
    "                         / (self.union * self.union)\n",
    "        # 判断 target 是否需要求梯度\n",
    "        if self.needs_input_grad[1]:\n",
    "            grad_target = None\n",
    "\n",
    "        return grad_input, grad_target\n",
    "\n",
    "\n",
    "def dice_coeff(input, target):\n",
    "    \"\"\"Dice coeff for batches\"\"\"\n",
    "    # 在合适的设备上初始化一个1*1零向量\n",
    "    # 同一个 batch 中 dice loss 取平均\n",
    "    s = torch.FloatTensor(1).cuda().zero_() if input.is_cuda else torch.FloatTensor(1).zero_()\n",
    "    for i, c in enumerate(zip(input, target)):\n",
    "        s = s + DiceCoeff().forward(c[0], c[1])\n",
    "    return s / (i + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_net(net, loader, device):\n",
    "    \"\"\"Evaluation without the densecrf with the dice coefficient\"\"\"\n",
    "    # 关闭 batchnorm 和 dropout\n",
    "    net.eval() # 仔细看\n",
    "    mask_type = torch.float32 if net.n_classes == 1 else torch.long\n",
    "    n_val = len(loader)  # the number of batch\n",
    "    tot = 0\n",
    "\n",
    "    # 括号里设置文字输出信息\n",
    "#     with tqdm(total = n_val, desc='Validation round', unit='batch', leave = False) as pbar:\n",
    "        # 对于每个 batch\n",
    "    for batch in loader:\n",
    "        imgs, true_masks = batch[0], batch[1]\n",
    "        imgs = imgs.to(device=device, dtype=torch.float32)\n",
    "        true_masks = true_masks.to(device=device, dtype=mask_type)\n",
    "\n",
    "        # 不需要追踪梯度变化，不需要进行反向传播，提升速度\n",
    "        with torch.no_grad():\n",
    "            # 得到模型预测结果\n",
    "            mask_pred = net(imgs)\n",
    "\n",
    "        # 不同类别预测结果损失累加\n",
    "        if net.n_classes > 1:\n",
    "            tot += F.cross_entropy(mask_pred, true_masks).item()\n",
    "        else:\n",
    "            pred = torch.sigmoid(mask_pred)\n",
    "            pred = (pred > 0.5).float()\n",
    "            tot += dice_coeff(pred, true_masks).item()\n",
    "#             pbar.update()\n",
    "\n",
    "    net.train()\n",
    "    return tot / n_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取图片及其标签路径（图片和标签配对）\n",
    "def read_images(root_path, train):\n",
    "    type_str = \"train\" if train else \"val\"\n",
    "    data = [root_path + \"/\" + type_str + \"/\" + file_name for file_name in list(os.walk(root_path+\"/\" + type_str + \"/\"))[0][2]]\n",
    "    file_names_order = [path.split(\"/\")[-1][:-1] for path in data]\n",
    "    labels = [root_path + \"/\" + type_str + \"_labels/\" + file_name for file_name in file_names_order]\n",
    "    return data, labels\n",
    "\n",
    "# 原图 0-255，tensor 中 0-1\n",
    "def change_to_tensor(img, label, tensor_size, output_size):\n",
    "    img = cv2.imread(img)\n",
    "    label = cv2.imread(label)\n",
    "    # 对于每一种类别\n",
    "    for i, cm in enumerate(colormap):\n",
    "        # 每个通道都符合条件（在本例子中实际只限制一个通道即可）\n",
    "        label[(label[:, :, 0] == cm[0]) & (label[:, :, 1] == cm[1]) & (label[:, :, 2] == cm[2])] = i * 255\n",
    "    label = label[:, :, 0] # 只取一个通道结果作为标签\n",
    "    \n",
    "    transform_data = transforms.Compose([transforms.Resize([tensor_size, tensor_size], 0),\n",
    "                                    transforms.ToTensor(),])\n",
    "    transform_label = transforms.Compose([transforms.Resize([output_size, output_size], 0),\n",
    "                                    transforms.ToTensor(),])\n",
    "    img_tensor, label_tensor = transform_data(Image.fromarray(img)), transform_label(Image.fromarray(label))\n",
    "    return img_tensor, label_tensor\n",
    "\n",
    "class RoadDataset(Dataset):\n",
    "    def __init__(self, root_path, train, input_size, output_size):\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        super(RoadDataset, self).__init__()\n",
    "        \n",
    "        self.data_list, self.label_list = read_images(root_path = root_path, train = train)\n",
    "        self.len = len(self.data_list)\n",
    "        print('Read '+ str(self.len)+' images')\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img = self.data_list[idx]\n",
    "        label = self.label_list[idx]\n",
    "        img, label = change_to_tensor(img, label, self.input_size, self.output_size)\n",
    "        return img, label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if ground truth is paired with corresponding data\n",
    "# img_list, lbl_list = read_images(road_path)\n",
    "# ll = []\n",
    "# for i, (img, lbl) in enumerate(zip(img_list, lbl_list)):\n",
    "#     ll.append(img.split(\"/\")[-1].split(\".\")[0] == lbl.split(\"/\")[-1].split(\".\")[0])\n",
    "# print(sum(ll), len(ll))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_net(net, device, train_dataset, val_dataset, epochs = EPOCH_NUM, lr = LR, save_cp = True,\n",
    "             batch_size = BATCH_SIZE):\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, BATCH_SIZE, shuffle = False)\n",
    "    val_loader = DataLoader(val_dataset, batch_size = 1, shuffle = False)\n",
    "    \n",
    "    # 每轮 evaluation 检验的 batch 个数\n",
    "    n_val = len(val_dataset)\n",
    "    # 每轮 train 检验的 batch 个数\n",
    "    n_train = len(train_dataset)\n",
    "\n",
    "    writer = SummaryWriter(comment=f'LR_{lr}_BS_{BATCH_SIZE}')\n",
    "    global_step = 0\n",
    "\n",
    "    logging.info(f'''Starting training:\n",
    "        Epochs:          {epochs}\n",
    "        Batch size:      {batch_size}\n",
    "        Learning rate:   {lr}\n",
    "        Training size:   {n_train}\n",
    "        Validation size: {n_val}\n",
    "        Checkpoints:     {save_cp}\n",
    "        Device:          {device.type}\n",
    "    ''')\n",
    "#     换 SGD，图像用 SGD Adam，收敛速度而非效果\n",
    "    optimizer = optim.RMSprop(net.parameters(), lr=lr, weight_decay=1e-8, momentum=0.9)\n",
    "#     optimizer = optim.SGD(net.parameters(), lr=lr, weight_decay=1e-8, momentum=0.9)\n",
    "    # 在发现loss不再降低或者acc不再提高之后，降低学习率。patience 含义：不再减小（或增大）的累计次数\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min' if net.n_classes > 1 else 'max', patience=2)\n",
    "    \n",
    "    if net.n_classes > 1:\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "    else:\n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        net.train()\n",
    "\n",
    "        epoch_loss = 0\n",
    "        with tqdm(total = n_train, desc=f'Epoch {epoch + 1}/{epochs}', unit='img') as pbar:\n",
    "            for batch in train_loader:\n",
    "                \n",
    "                imgs = batch[0]\n",
    "                true_masks = batch[1]\n",
    "                \n",
    "                assert imgs.shape[1] == net.n_channels, \\\n",
    "                    f'Network has been defined with {net.n_channels} input channels, ' \\\n",
    "                    f'but loaded images have {imgs.shape[1]} channels. Please check that ' \\\n",
    "                    'the images are loaded correctly.'\n",
    "\n",
    "                imgs = imgs.to(device = device, dtype = torch.float32)\n",
    "                mask_type = torch.float32 if net.n_classes == 1 else torch.long\n",
    "                true_masks = true_masks.to(device = device, dtype=mask_type)\n",
    "\n",
    "                masks_pred = net(imgs)\n",
    "                loss = criterion(masks_pred, true_masks)\n",
    "                epoch_loss += loss.item()\n",
    "                writer.add_scalar('Loss/train', loss.item(), global_step)\n",
    "\n",
    "                pbar.set_postfix(**{'loss (batch)': loss.item()})\n",
    "\n",
    "                # 对于每个 batch 都要更新一次参数空间\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                # 防止梯度爆炸，设置梯度截断\n",
    "                nn.utils.clip_grad_value_(net.parameters(), 0.1)\n",
    "                optimizer.step()\n",
    "\n",
    "                # 每个 batch 结束更新一次进度条，迭代器内部计数器累加 batch 的大小\n",
    "                pbar.update(imgs.shape[0])\n",
    "                global_step += 1\n",
    "                \n",
    "                # 每训练 10 个 batch 在 tensorboard 中记录一次\n",
    "                if global_step % (n_train // (10 * batch_size) + 1) == 0:\n",
    "                    for tag, value in net.named_parameters():\n",
    "                        tag = tag.replace('.', '/')\n",
    "                        writer.add_histogram('weights/' + tag, value.data.cpu().numpy(), global_step)\n",
    "                        writer.add_histogram('grads/' + tag, value.grad.data.cpu().numpy(), global_step)\n",
    "                    val_score = eval_net(net, val_loader, device)\n",
    "                    scheduler.step(val_score)\n",
    "                    writer.add_scalar('learning_rate', optimizer.param_groups[0]['lr'], global_step)\n",
    "\n",
    "                    if net.n_classes > 1:\n",
    "                        logging.info('Validation cross entropy: {}'.format(val_score))\n",
    "                        writer.add_scalar('Loss/test', val_score, global_step)\n",
    "                    else:\n",
    "                        logging.info('Validation Dice Coeff: {}'.format(val_score))\n",
    "                        writer.add_scalar('Dice/test', val_score, global_step)\n",
    "\n",
    "                    writer.add_images('images', imgs, global_step)\n",
    "                    if net.n_classes == 1:\n",
    "                        writer.add_images('masks/true', true_masks, global_step)\n",
    "                        writer.add_images('masks/pred', torch.sigmoid(masks_pred) > 0.5, global_step)\n",
    "\n",
    "        if save_cp:\n",
    "            try:\n",
    "                os.mkdir(DIR_CHECKPOINT)\n",
    "                logging.info('Created checkpoint directory')\n",
    "            except OSError:\n",
    "                pass\n",
    "            torch.save(net.state_dict(),\n",
    "                       DIR_CHECKPOINT + f'unet_epoch{epoch + 1}.pth')\n",
    "            logging.info(f'Checkpoint {epoch + 1} saved !')\n",
    "            if os.path.exists(DIR_CHECKPOINT + f'unet_epoch{epoch - 4}.pth') & (epoch - 4)//10 != 0:\n",
    "                os.remove(DIR_CHECKPOINT + f'unet_epoch{epoch - 4}.pth')\n",
    "                logging.info(f'Checkpoint {epoch - 4} deleted !')\n",
    "\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# net = UNet(n_channels = 3, n_classes = 1, bilinear = False)\n",
    "# t = net(img_tensor.unsqueeze(0))\n",
    "# t = (torch.sigmoid(t).squeeze(0) > 0.5) * 255\n",
    "# t = t.detach().numpy()\n",
    "# t = np.concatenate([t, t, t])\n",
    "# t = t.transpose((1, 2, 0))\n",
    "# plt.imshow(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check\n",
    "# import pandas as pd\n",
    "# pd.DataFrame({\"img\": [t.split(\"/\")[-1] for t in train_dataset.data_list],\n",
    "#               \"lbl\": [t.split(\"/\")[-1] for t in train_dataset.label_list]}).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 366 images\n",
      "Read 56 images\n"
     ]
    }
   ],
   "source": [
    "net = UNet(n_channels = 3, n_classes = 1, bilinear = False).to(device)\n",
    "\n",
    "train_dataset = RoadDataset(road_path, True, INPUT_SIZE, OUTPUT_SIZE)\n",
    "val_dataset = RoadDataset(road_path, False, INPUT_SIZE, OUTPUT_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100: 100%|██████████| 366/366 [03:16<00:00,  1.86img/s, loss (batch)=0.461]\n",
      "Epoch 2/100: 100%|██████████| 366/366 [03:17<00:00,  1.86img/s, loss (batch)=0.46] \n",
      "Epoch 3/100: 100%|██████████| 366/366 [03:15<00:00,  1.87img/s, loss (batch)=0.46] \n",
      "Epoch 4/100: 100%|██████████| 366/366 [03:13<00:00,  1.89img/s, loss (batch)=0.46] \n",
      "Epoch 5/100: 100%|██████████| 366/366 [03:28<00:00,  1.76img/s, loss (batch)=0.46] \n",
      "Epoch 6/100:  22%|██▏       | 80/366 [00:30<01:05,  4.36img/s, loss (batch)=0.518]"
     ]
    }
   ],
   "source": [
    "train_net(net, device, train_dataset, val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

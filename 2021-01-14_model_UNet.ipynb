{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from torch.autograd import Function\n",
    "from torchsummary import summary\n",
    "\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "from unet import UNet\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader, random_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# U-Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCH_NUM = 1\n",
    "BATCH_SIZE = 2\n",
    "LR = 0.001\n",
    "INPUT_SIZE = 512 # 输入网络的大小而非图片大小\n",
    "OUTPUT_SIZE = 324\n",
    "\n",
    "root_path = 'D://Data/massachusetts-roads-dataset/'\n",
    "# root_path = '/home/renyan/ossdata/massachusetts-roads-dataset/'\n",
    "road_path = root_path + \"tiff_select_parts/\"\n",
    "\n",
    "DIR_CHECKPOINT = 'checkpoints/'\n",
    "\n",
    "classes = ['background', 'road']\n",
    "colormap = [[0 , 0, 0], [255, 255, 255]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleConv(nn.Module):\n",
    "    # 输入输出的 feature map 个数可以控制\n",
    "    # 但是图片具体大小根据图片本身大小决定\n",
    "    def __init__(self, in_channels, out_channels, mid_channels=None):\n",
    "        super().__init__()\n",
    "        if not mid_channels:\n",
    "            mid_channels = out_channels\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=0),\n",
    "            nn.BatchNorm2d(mid_channels),\n",
    "            nn.ReLU(inplace = True),\n",
    "            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=0),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace = True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Down(nn.Module):\n",
    "    \"\"\"Downscaling with maxpool then double conv\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.maxpool_conv = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            DoubleConv(in_channels, out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.maxpool_conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Up(nn.Module):\n",
    "    \"\"\"Upscaling then double conv\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, bilinear=True):\n",
    "        super().__init__()\n",
    "\n",
    "        # if bilinear, use the normal convolutions to reduce the number of channels\n",
    "        if bilinear:\n",
    "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "            self.conv = DoubleConv(in_channels, out_channels, in_channels // 2)\n",
    "        else:\n",
    "            self.up = nn.ConvTranspose2d(in_channels , in_channels // 2, kernel_size=2, stride=2)\n",
    "            self.conv = DoubleConv(in_channels, out_channels)\n",
    "\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "        diffY = x2.size()[2] - x1.size()[2]\n",
    "        diffX = x2.size()[3] - x1.size()[3]\n",
    "\n",
    "#         x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
    "#                         diffY // 2, diffY - diffY // 2])\n",
    "        x2 = x2[:,:, (diffY // 2): -(diffY - diffY // 2), (diffY // 2): -(diffX - diffX // 2)]\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        return self.conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OutConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(OutConv, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self, n_channels, n_classes, bilinear=True):\n",
    "        super(UNet, self).__init__()\n",
    "        self.n_channels = n_channels # 图片的通道数（最初输入）\n",
    "        self.n_classes = n_classes # 最终类别墅（最终输出）\n",
    "        self.bilinear = bilinear # -- 暂不用\n",
    "\n",
    "        self.inc = DoubleConv(n_channels, 64)\n",
    "        self.down1 = Down(64, 128)\n",
    "        self.down2 = Down(128, 256)\n",
    "        self.down3 = Down(256, 512)\n",
    "        factor = 2 if bilinear else 1\n",
    "        self.down4 = Down(512, 1024 // factor)\n",
    "        self.up1 = Up(1024, 512 // factor, bilinear)\n",
    "        self.up2 = Up(512, 256 // factor, bilinear)\n",
    "        self.up3 = Up(256, 128 // factor, bilinear)\n",
    "        self.up4 = Up(128, 64, bilinear)\n",
    "        self.outc = OutConv(64, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "        x = self.up1(x5, x4)\n",
    "        x = self.up2(x, x3)\n",
    "        x = self.up3(x, x2)\n",
    "        x = self.up4(x, x1)\n",
    "        logits = self.outc(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tnet = UNet(3, 1, False)\n",
    "# summary(tnet, (3, 512, 512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('D:/Data/massachusetts-roads-dataset/tiff_select_parts/train/10078660_15_p1.tiff')\n",
    "label = cv2.imread('D:/Data/massachusetts-roads-dataset/tiff_select_parts/train_labels/10078660_15_p1.tif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((750, 750, 3), (750, 750, 3))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img.shape, label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.Resize([512, 512], 0),\n",
    "                                transforms.ToTensor(),])\n",
    "img_tensor, label_tensor = transform(Image.fromarray(img)), transform(Image.fromarray(label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(img_tensor.permute(1, 2, 0).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(label_tensor.permute(1, 2, 0).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Dice coefficient: \n",
    "    - measure the similarity between two set.\n",
    "    - $\\frac{2|X\\cap Y|}{|X| + |Y|}$\n",
    "- Dice loss: $1 - \\frac{2|X\\cap Y|}{|X| + |Y|}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiceCoeff(Function):\n",
    "    \"\"\"Dice coeff for individual examples\"\"\"\n",
    "\n",
    "    # 在进入 forward 之前，所有变量都会被转化为 tensor\n",
    "    def forward(self, input, target):\n",
    "        self.save_for_backward(input, target) # tensor 转化为变量保存到后续操作\n",
    "        eps = 0.0001\n",
    "        self.inter = torch.dot(input.view(-1), target.view(-1))\n",
    "        self.union = torch.sum(input) + torch.sum(target) + eps\n",
    "\n",
    "        t = (2 * self.inter.float() + eps) / self.union.float()\n",
    "        return t\n",
    "\n",
    "    # This function has only a single output, so it gets only one gradient\n",
    "    def backward(self, grad_output):\n",
    "        input, target = self.saved_variables\n",
    "        grad_input = grad_target = None\n",
    "\n",
    "        # 判断 input 是否需要求梯度\n",
    "        if self.needs_input_grad[0]:\n",
    "            grad_input = grad_output * 2 * (target * self.union - self.inter) \\\n",
    "                         / (self.union * self.union)\n",
    "        # 判断 target 是否需要求梯度\n",
    "        if self.needs_input_grad[1]:\n",
    "            grad_target = None\n",
    "\n",
    "        return grad_input, grad_target\n",
    "\n",
    "\n",
    "def dice_coeff(input, target):\n",
    "    \"\"\"Dice coeff for batches\"\"\"\n",
    "    # 在合适的设备上初始化一个1*1零向量\n",
    "    # 同一个 batch 中 dice loss 取平均\n",
    "    s = torch.FloatTensor(1).cuda().zero_() if input.is_cuda else torch.FloatTensor(1).zero_()\n",
    "    for i, c in enumerate(zip(input, target)):\n",
    "        s = s + DiceCoeff().forward(c[0], c[1])\n",
    "    return s / (i + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_net(net, loader, device):\n",
    "    \"\"\"Evaluation without the densecrf with the dice coefficient\"\"\"\n",
    "    # 关闭 batchnorm 和 dropout\n",
    "    net.eval() # 仔细看\n",
    "    mask_type = torch.float32 if net.n_classes == 1 else torch.long\n",
    "    n_val = len(loader)  # the number of batch\n",
    "    tot = 0\n",
    "\n",
    "    # 括号里设置文字输出信息\n",
    "    with tqdm(total = n_val, desc='Validation round', unit='batch', leave=False) as pbar:\n",
    "        # 对于每个 batch\n",
    "        for batch in loader:\n",
    "            imgs, true_masks = batch[0], batch[1]\n",
    "            imgs = imgs.to(device=device, dtype=torch.float32)\n",
    "            true_masks = true_masks.to(device=device, dtype=mask_type)\n",
    "\n",
    "            # 不需要追踪梯度变化，不需要进行反向传播，提升速度\n",
    "            with torch.no_grad():\n",
    "                # 得到模型预测结果\n",
    "                mask_pred = net(imgs)\n",
    "\n",
    "            # 不同类别预测结果损失累加\n",
    "            if net.n_classes > 1:\n",
    "                tot += F.cross_entropy(mask_pred, true_masks).item()\n",
    "            else:\n",
    "                pred = torch.sigmoid(mask_pred)\n",
    "                pred = (pred > 0.5).float()\n",
    "                tot += dice_coeff(pred, true_masks).item()\n",
    "            pbar.update()\n",
    "\n",
    "    net.train()\n",
    "    return tot / n_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取图片及其标签路径（图片和标签配对）\n",
    "def read_images(root_path, train = True):\n",
    "    if train:\n",
    "        data = [root_path + \"/train/\" + file_name for file_name in list(os.walk(road_path+\"/train/\"))[0][2]]\n",
    "        labels = [root_path + \"/train_labels/\" + file_name for file_name in list(os.walk(road_path+\"/train_labels/\"))[0][2]]\n",
    "    else:\n",
    "        data = [root_path + \"/val/\" +file_name for file_name in list(os.walk(road_path+\"/val/\"))[0][2]]\n",
    "        labels = [root_path + \"/val_labels/\" + file_name for file_name in list(os.walk(road_path+\"/val_labels/\"))[0][2]]\n",
    "    return data, labels\n",
    "\n",
    "# 原图 0-255，tensor 中 0-1\n",
    "def change_to_tensor(img, label, tensor_size, output_size):\n",
    "    img = cv2.imread(img)\n",
    "    label = cv2.imread(label)\n",
    "    # 对于每一种类别\n",
    "    for i, cm in enumerate(colormap):\n",
    "        # 每个通道都符合条件（在本例子中实际只限制一个通道即可）\n",
    "        label[(label[:, :, 0] == cm[0]) & (label[:, :, 1] == cm[1]) & (label[:, :, 2] == cm[2])] = i * 255\n",
    "    label = label[:, :, 0] # 只取一个通道结果作为标签\n",
    "    \n",
    "    transform_data = transforms.Compose([transforms.Resize([tensor_size, tensor_size], 0),\n",
    "                                    transforms.ToTensor(),])\n",
    "    transform_label = transforms.Compose([transforms.Resize([output_size, output_size], 0),\n",
    "                                    transforms.ToTensor(),])\n",
    "    img_tensor, label_tensor = transform_data(Image.fromarray(img)), transform_label(Image.fromarray(label))\n",
    "    return img_tensor, label_tensor\n",
    "\n",
    "class RoadDataset(Dataset):\n",
    "    def __init__(self, root_path, train, input_size, output_size):\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        super(RoadDataset, self).__init__()\n",
    "        \n",
    "        self.data_list, self.label_list = read_images(root_path = root_path, train = train)\n",
    "        self.len = len(self.data_list)\n",
    "        print('Read '+ str(self.len)+' images')\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img = self.data_list[idx]\n",
    "        label = self.label_list[idx]\n",
    "        img, label = change_to_tensor(img, label, self.input_size, self.output_size)\n",
    "        return img, label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_net(net, device, train_dataset, val_dataset, epochs = EPOCH_NUM, lr = LR, save_cp = True,\n",
    "             batch_size = BATCH_SIZE):\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, BATCH_SIZE, shuffle=False)\n",
    "    val_loader = DataLoader(val_dataset, batch_size = 1, shuffle=False)\n",
    "    \n",
    "    # 每轮 evaluation 检验的 batch 个数\n",
    "    n_val = len(val_dataset)\n",
    "    # 每轮 train 检验的 batch 个数\n",
    "    n_train = len(train_dataset)\n",
    "\n",
    "    writer = SummaryWriter(comment=f'LR_{lr}_BS_{BATCH_SIZE}')\n",
    "    global_step = 0\n",
    "\n",
    "    logging.info(f'''Starting training:\n",
    "        Epochs:          {epochs}\n",
    "        Batch size:      {batch_size}\n",
    "        Learning rate:   {lr}\n",
    "        Training size:   {n_train}\n",
    "        Validation size: {n_val}\n",
    "        Checkpoints:     {save_cp}\n",
    "        Device:          {device.type}\n",
    "    ''')\n",
    "#     换 SGD，图像用 SGD Adam，收敛速度而非效果\n",
    "#     optimizer = optim.RMSprop(net.parameters(), lr=lr, weight_decay=1e-8, momentum=0.9)\n",
    "    optimizer = optim.SGD(net.parameters(), lr=lr, weight_decay=1e-8, momentum=0.9)\n",
    "    # 在发现loss不再降低或者acc不再提高之后，降低学习率。patience 含义：不再减小（或增大）的累计次数\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min' if net.n_classes > 1 else 'max', patience=2)\n",
    "    \n",
    "    if net.n_classes > 1:\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "    else:\n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        net.train()\n",
    "\n",
    "        epoch_loss = 0\n",
    "        with tqdm(total = n_train, desc=f'Epoch {epoch + 1}/{epochs}', unit='img') as pbar:\n",
    "            for batch in train_loader:\n",
    "                \n",
    "                imgs = batch[0]\n",
    "                true_masks = batch[1]\n",
    "                \n",
    "                assert imgs.shape[1] == net.n_channels, \\\n",
    "                    f'Network has been defined with {net.n_channels} input channels, ' \\\n",
    "                    f'but loaded images have {imgs.shape[1]} channels. Please check that ' \\\n",
    "                    'the images are loaded correctly.'\n",
    "\n",
    "                imgs = imgs.to(device=device, dtype=torch.float32)\n",
    "                mask_type = torch.float32 if net.n_classes == 1 else torch.long\n",
    "                true_masks = true_masks.to(device=device, dtype=mask_type)\n",
    "\n",
    "                masks_pred = net(imgs)\n",
    "                loss = criterion(masks_pred, true_masks)\n",
    "                epoch_loss += loss.item()\n",
    "                writer.add_scalar('Loss/train', loss.item(), global_step)\n",
    "\n",
    "                pbar.set_postfix(**{'loss (batch)': loss.item()})\n",
    "\n",
    "                # 对于每个 batch 都要更新一次参数空间\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                # 防止梯度爆炸，设置梯度截断\n",
    "                nn.utils.clip_grad_value_(net.parameters(), 0.1)\n",
    "                optimizer.step()\n",
    "\n",
    "                # 每个 batch 结束更新一次进度条，迭代器内部计数器累加 batch 的大小\n",
    "                pbar.update(imgs.shape[0])\n",
    "                global_step += 1\n",
    "                \n",
    "                # 每训练 10 个 batch 在 tensorboard 中记录一次\n",
    "                if global_step % (n_train // (10 * batch_size) + 1) == 0:\n",
    "                    for tag, value in net.named_parameters():\n",
    "                        tag = tag.replace('.', '/')\n",
    "                        writer.add_histogram('weights/' + tag, value.data.cpu().numpy(), global_step)\n",
    "                        writer.add_histogram('grads/' + tag, value.grad.data.cpu().numpy(), global_step)\n",
    "                    val_score = eval_net(net, val_loader, device)\n",
    "                    scheduler.step(val_score)\n",
    "                    writer.add_scalar('learning_rate', optimizer.param_groups[0]['lr'], global_step)\n",
    "\n",
    "                    if net.n_classes > 1:\n",
    "                        logging.info('Validation cross entropy: {}'.format(val_score))\n",
    "                        writer.add_scalar('Loss/test', val_score, global_step)\n",
    "                    else:\n",
    "                        logging.info('Validation Dice Coeff: {}'.format(val_score))\n",
    "                        writer.add_scalar('Dice/test', val_score, global_step)\n",
    "\n",
    "                    writer.add_images('images', imgs, global_step)\n",
    "                    if net.n_classes == 1:\n",
    "                        writer.add_images('masks/true', true_masks, global_step)\n",
    "                        writer.add_images('masks/pred', torch.sigmoid(masks_pred) > 0.5, global_step)\n",
    "\n",
    "        if save_cp:\n",
    "            try:\n",
    "                os.mkdir(DIR_CHECKPOINT)\n",
    "                logging.info('Created checkpoint directory')\n",
    "            except OSError:\n",
    "                pass\n",
    "            torch.save(net.state_dict(),\n",
    "                       DIR_CHECKPOINT + f'unet_epoch{epoch + 1}.pth')\n",
    "            logging.info(f'Checkpoint {epoch + 1} saved !')\n",
    "\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# net = UNet(n_channels = 3, n_classes = 1, bilinear = False)\n",
    "# t = net(img_tensor.unsqueeze(0))\n",
    "# t = (torch.sigmoid(t).squeeze(0) > 0.5) * 255\n",
    "# t = t.detach().numpy()\n",
    "# t = np.concatenate([t, t, t])\n",
    "# t = t.transpose((1, 2, 0))\n",
    "# plt.imshow(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# net = UNet(n_channels = 3, n_classes = 1, bilinear = False)\n",
    "# t = net(img_tensor.unsqueeze(0))\n",
    "# t = (torch.sigmoid(t).squeeze(0) > 0.5) * 255\n",
    "# t = t.detach().numpy()\n",
    "# t = np.concatenate([t, t, t])\n",
    "# t = t.transpose((1, 2, 0))\n",
    "# plt.imshow(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 368 images\n",
      "Read 56 images\n"
     ]
    }
   ],
   "source": [
    "net = UNet(n_channels = 3, n_classes = 1, bilinear = False)\n",
    "device = torch.device('cuda' if torch.\n",
    "                      cuda.is_available() else 'cpu')\n",
    "if(device == 'cuda'):\n",
    "    net = net.cuda()\n",
    "\n",
    "train_dataset = RoadDataset(road_path, True, INPUT_SIZE, OUTPUT_SIZE)\n",
    "val_dataset = RoadDataset(road_path, False, INPUT_SIZE, OUTPUT_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:   4%|▍         | 16/368 [01:14<27:19,  4.66s/img, loss (batch)=0.643]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-114-228c6c3b4e61>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain_net\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_dataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-29-457a5cc29cd0>\u001b[0m in \u001b[0;36mtrain_net\u001b[1;34m(net, device, train_dataset, val_dataset, epochs, lr, save_cp, batch_size)\u001b[0m\n\u001b[0;32m     52\u001b[0m                 \u001b[0mtrue_masks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrue_masks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmask_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m                 \u001b[0mmasks_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     55\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmasks_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrue_masks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m                 \u001b[0mepoch_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    721\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 722\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-63-1b347b92bf85>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0mx1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m         \u001b[0mx2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdown1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m         \u001b[0mx3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdown2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[0mx4\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdown3\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    721\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 722\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-49a8e69e9c8a>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmaxpool_conv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    721\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 722\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    115\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 117\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    118\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    721\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 722\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-8a734eb1f941>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdouble_conv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    721\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 722\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    115\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 117\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    118\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    721\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 722\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    417\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    418\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 419\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    420\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    421\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[1;34m(self, input, weight)\u001b[0m\n\u001b[0;32m    414\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m    415\u001b[0m         return F.conv2d(input, weight, self.bias, self.stride,\n\u001b[1;32m--> 416\u001b[1;33m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[0;32m    417\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    418\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_net(net, device, train_dataset, val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), DIR_CHECKPOINT + 'unet_epoch1.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), DIR_CHECKPOINT + 'unet_epoch2.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = UNet(n_channels=3, n_classes=1, bilinear=True)\n",
    "net.load_state_dict(torch.load(DIR_CHECKPOINT + 'unet_epoch1.pth'))\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "net = net.to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary(net, (3, 578, 578))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_img(net, img, device):\n",
    "    net.eval()\n",
    "    if len(img.shape) == 3:\n",
    "        img = img.unsqueeze(0)\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        if net.n_classes > 1:\n",
    "            pred = net(img.unsqueeze(dim=0))\n",
    "            pred = F.softmax(pred, dim=1).max(dim = 1)[1]\n",
    "            pred_img = torch.cat((pred, pred, pred))\n",
    "            pred_img = pred_img.permute((1, 2, 0)) * 255\n",
    "        else:\n",
    "            pred = net(img)\n",
    "            pred = (torch.sigmoid(pred).squeeze(0) > 0.5) * 255\n",
    "            pred = pred.detach().numpy()\n",
    "            pred_img = np.concatenate([pred, pred, pred]).transpose((1, 2, 0))\n",
    "            \n",
    "    return pred_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = predict_img(net, img_tensor, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(388, 388, 3)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x263488d8288>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9404825167392922"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.flatten().sum().item()/pred.flatten().shape[0]/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 388, 388])"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.sha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 578, 578])"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = 'D:/Data/massachusetts-roads-dataset/tiff_excel/train/17428900_15.tiff'\n",
    "label = 'D:/Data/massachusetts-roads-dataset/tiff_excel/train_labels/17428900_15.tif'\n",
    "img, label = change_to_tensor(img, label, INPUT_SIZE)\n",
    "img.unsqueeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = net(img.unsqueeze(dim=0))\n",
    "t = F.softmax(pred, dim=1)\n",
    "t = t.max(dim=1)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 388, 388])"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 388, 388])"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat((t, t)).unsqueeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.max(\n",
       "values=tensor([[[0.5004, 0.5005, 0.5009,  ..., 0.5005, 0.5007, 0.5005],\n",
       "         [0.5006, 0.5006, 0.5002,  ..., 0.5011, 0.5007, 0.5003],\n",
       "         [0.5001, 0.5003, 0.5006,  ..., 0.5003, 0.5005, 0.5005],\n",
       "         ...,\n",
       "         [0.5000, 0.5002, 0.5002,  ..., 0.5009, 0.5010, 0.5010],\n",
       "         [0.5002, 0.5001, 0.5001,  ..., 0.5007, 0.5009, 0.5009],\n",
       "         [0.5002, 0.5008, 0.5005,  ..., 0.5011, 0.5012, 0.5007]]],\n",
       "       grad_fn=<MaxBackward0>),\n",
       "indices=tensor([[[1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [0, 1, 0,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1]]]))"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = net(img.unsqueeze(dim = 0))\n",
    "t = F.softmax(pred, dim=1)\n",
    "t.max(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 388, 388])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.squeeze().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.max(\n",
       "values=tensor([[0.4468, 0.4418, 0.5084,  ..., 0.4056, 0.4508, 0.4752],\n",
       "        [0.4375, 0.4722, 0.5347,  ..., 0.4802, 0.4503, 0.5240],\n",
       "        [0.4584, 0.4502, 0.4986,  ..., 0.5917, 0.4890, 0.4494],\n",
       "        ...,\n",
       "        [0.4935, 0.3981, 0.4046,  ..., 0.5127, 0.4666, 0.4579],\n",
       "        [0.5058, 0.4685, 0.4194,  ..., 0.4363, 0.4114, 0.4734],\n",
       "        [0.4549, 0.4499, 0.4640,  ..., 0.4177, 0.4350, 0.3620]],\n",
       "       grad_fn=<MaxBackward0>),\n",
       "indices=tensor([[0, 1, 1,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 1, 1, 1],\n",
       "        ...,\n",
       "        [1, 1, 0,  ..., 1, 1, 1],\n",
       "        [1, 1, 0,  ..., 1, 1, 1],\n",
       "        [1, 1, 0,  ..., 1, 1, 1]]))"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.squeeze(0).max(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

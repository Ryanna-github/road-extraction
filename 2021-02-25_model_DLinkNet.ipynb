{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# D-LinkNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import optim\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Function\n",
    "from torchvision import models\n",
    "import torch.nn.functional as F\n",
    "from functools import partial\n",
    "import os\n",
    "\n",
    "import data_loader\n",
    "from torchsummary import summary\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import logging\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SIZE, OUTPUT_SIZE = 256, 256\n",
    "# root_path = 'D://Data/massachusetts-roads-dataset/'\n",
    "root_path = '/home/renyan/ossdata/massachusetts-roads-dataset/'\n",
    "road_path = root_path + \"tiff_select2_parts_16/\"\n",
    "\n",
    "DIR_CHECKPOINT = 'checkpoints/'\n",
    "\n",
    "BATCH_SIZE = 4\n",
    "EPOCH_NUM = 20\n",
    "LR = 0.0002"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dblock(nn.Module):\n",
    "    def __init__(self,channel):\n",
    "        super(Dblock, self).__init__()\n",
    "        self.nonlinearity = partial(F.relu,inplace = True)\n",
    "        self.dilate1 = nn.Conv2d(channel, channel, kernel_size=3, dilation=1, padding=1)\n",
    "        self.dilate2 = nn.Conv2d(channel, channel, kernel_size=3, dilation=2, padding=2)\n",
    "        self.dilate3 = nn.Conv2d(channel, channel, kernel_size=3, dilation=4, padding=4)\n",
    "        self.dilate4 = nn.Conv2d(channel, channel, kernel_size=3, dilation=8, padding=8)\n",
    "        #self.dilate5 = nn.Conv2d(channel, channel, kernel_size=3, dilation=16, padding=16)\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.zero_()\n",
    "                    \n",
    "    def forward(self, x):\n",
    "        dilate1_out = self.nonlinearity(self.dilate1(x))\n",
    "        dilate2_out = self.nonlinearity(self.dilate2(dilate1_out))\n",
    "        dilate3_out = self.nonlinearity(self.dilate3(dilate2_out))\n",
    "        dilate4_out = self.nonlinearity(self.dilate4(dilate3_out))\n",
    "        #dilate5_out = nonlinearity(self.dilate5(dilate4_out))\n",
    "        out = x + dilate1_out + dilate2_out + dilate3_out + dilate4_out# + dilate5_out\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, in_channels, n_filters):\n",
    "        super(DecoderBlock,self).__init__()\n",
    "        self.nonlinearity = partial(F.relu,inplace=True)\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels, in_channels // 4, 1)\n",
    "        self.norm1 = nn.BatchNorm2d(in_channels // 4)\n",
    "        self.relu1 = self.nonlinearity\n",
    "\n",
    "        self.deconv2 = nn.ConvTranspose2d(in_channels // 4, in_channels // 4, 3, stride=2, padding=1, output_padding=1)\n",
    "        self.norm2 = nn.BatchNorm2d(in_channels // 4)\n",
    "        self.relu2 = self.nonlinearity\n",
    "\n",
    "        self.conv3 = nn.Conv2d(in_channels // 4, n_filters, 1)\n",
    "        self.norm3 = nn.BatchNorm2d(n_filters)\n",
    "        self.relu3 = self.nonlinearity\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.norm1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.deconv2(x)\n",
    "        x = self.norm2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.norm3(x)\n",
    "        x = self.relu3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DLinkNet34(nn.Module):\n",
    "    def __init__(self, num_classes = 1):\n",
    "        super(DLinkNet34, self).__init__()\n",
    "        self.nonlinearity = partial(F.relu,inplace = True)\n",
    "        self.n_classes = num_classes\n",
    "\n",
    "        filters = [64, 128, 256, 512]\n",
    "        resnet = models.resnet34(pretrained = True)\n",
    "        self.firstconv = resnet.conv1\n",
    "        self.firstbn = resnet.bn1\n",
    "        self.firstrelu = resnet.relu\n",
    "        self.firstmaxpool = resnet.maxpool\n",
    "        self.encoder1 = resnet.layer1\n",
    "        self.encoder2 = resnet.layer2\n",
    "        self.encoder3 = resnet.layer3\n",
    "        self.encoder4 = resnet.layer4\n",
    "        \n",
    "        self.dblock = Dblock(512)\n",
    "\n",
    "        self.decoder4 = DecoderBlock(filters[3], filters[2])\n",
    "        self.decoder3 = DecoderBlock(filters[2], filters[1])\n",
    "        self.decoder2 = DecoderBlock(filters[1], filters[0])\n",
    "        self.decoder1 = DecoderBlock(filters[0], filters[0])\n",
    "\n",
    "        self.finaldeconv1 = nn.ConvTranspose2d(filters[0], 32, 4, 2, 1)\n",
    "        self.finalrelu1 = self.nonlinearity\n",
    "        self.finalconv2 = nn.Conv2d(32, 32, 3, padding=1)\n",
    "        self.finalrelu2 = self.nonlinearity\n",
    "        self.finalconv3 = nn.Conv2d(32, self.n_classes, 3, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        x = self.firstconv(x)\n",
    "        x = self.firstbn(x)\n",
    "        x = self.firstrelu(x)\n",
    "        x = self.firstmaxpool(x)\n",
    "        e1 = self.encoder1(x)\n",
    "        e2 = self.encoder2(e1)\n",
    "        e3 = self.encoder3(e2)\n",
    "        e4 = self.encoder4(e3)\n",
    "        \n",
    "        # Center\n",
    "        e4 = self.dblock(e4)\n",
    "\n",
    "        # Decoder\n",
    "        d4 = self.decoder4(e4) + e3\n",
    "        d3 = self.decoder3(d4) + e2\n",
    "        d2 = self.decoder2(d3) + e1\n",
    "        d1 = self.decoder1(d2)\n",
    "        \n",
    "        out = self.finaldeconv1(d1)\n",
    "        out = self.finalrelu1(out)\n",
    "        out = self.finalconv2(out)\n",
    "        out = self.finalrelu2(out)\n",
    "        out = self.finalconv3(out)\n",
    "\n",
    "        return torch.sigmoid(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = DLinkNet34()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 128, 128]           9,408\n",
      "       BatchNorm2d-2         [-1, 64, 128, 128]             128\n",
      "              ReLU-3         [-1, 64, 128, 128]               0\n",
      "         MaxPool2d-4           [-1, 64, 64, 64]               0\n",
      "            Conv2d-5           [-1, 64, 64, 64]          36,864\n",
      "       BatchNorm2d-6           [-1, 64, 64, 64]             128\n",
      "              ReLU-7           [-1, 64, 64, 64]               0\n",
      "            Conv2d-8           [-1, 64, 64, 64]          36,864\n",
      "       BatchNorm2d-9           [-1, 64, 64, 64]             128\n",
      "             ReLU-10           [-1, 64, 64, 64]               0\n",
      "       BasicBlock-11           [-1, 64, 64, 64]               0\n",
      "           Conv2d-12           [-1, 64, 64, 64]          36,864\n",
      "      BatchNorm2d-13           [-1, 64, 64, 64]             128\n",
      "             ReLU-14           [-1, 64, 64, 64]               0\n",
      "           Conv2d-15           [-1, 64, 64, 64]          36,864\n",
      "      BatchNorm2d-16           [-1, 64, 64, 64]             128\n",
      "             ReLU-17           [-1, 64, 64, 64]               0\n",
      "       BasicBlock-18           [-1, 64, 64, 64]               0\n",
      "           Conv2d-19           [-1, 64, 64, 64]          36,864\n",
      "      BatchNorm2d-20           [-1, 64, 64, 64]             128\n",
      "             ReLU-21           [-1, 64, 64, 64]               0\n",
      "           Conv2d-22           [-1, 64, 64, 64]          36,864\n",
      "      BatchNorm2d-23           [-1, 64, 64, 64]             128\n",
      "             ReLU-24           [-1, 64, 64, 64]               0\n",
      "       BasicBlock-25           [-1, 64, 64, 64]               0\n",
      "           Conv2d-26          [-1, 128, 32, 32]          73,728\n",
      "      BatchNorm2d-27          [-1, 128, 32, 32]             256\n",
      "             ReLU-28          [-1, 128, 32, 32]               0\n",
      "           Conv2d-29          [-1, 128, 32, 32]         147,456\n",
      "      BatchNorm2d-30          [-1, 128, 32, 32]             256\n",
      "           Conv2d-31          [-1, 128, 32, 32]           8,192\n",
      "      BatchNorm2d-32          [-1, 128, 32, 32]             256\n",
      "             ReLU-33          [-1, 128, 32, 32]               0\n",
      "       BasicBlock-34          [-1, 128, 32, 32]               0\n",
      "           Conv2d-35          [-1, 128, 32, 32]         147,456\n",
      "      BatchNorm2d-36          [-1, 128, 32, 32]             256\n",
      "             ReLU-37          [-1, 128, 32, 32]               0\n",
      "           Conv2d-38          [-1, 128, 32, 32]         147,456\n",
      "      BatchNorm2d-39          [-1, 128, 32, 32]             256\n",
      "             ReLU-40          [-1, 128, 32, 32]               0\n",
      "       BasicBlock-41          [-1, 128, 32, 32]               0\n",
      "           Conv2d-42          [-1, 128, 32, 32]         147,456\n",
      "      BatchNorm2d-43          [-1, 128, 32, 32]             256\n",
      "             ReLU-44          [-1, 128, 32, 32]               0\n",
      "           Conv2d-45          [-1, 128, 32, 32]         147,456\n",
      "      BatchNorm2d-46          [-1, 128, 32, 32]             256\n",
      "             ReLU-47          [-1, 128, 32, 32]               0\n",
      "       BasicBlock-48          [-1, 128, 32, 32]               0\n",
      "           Conv2d-49          [-1, 128, 32, 32]         147,456\n",
      "      BatchNorm2d-50          [-1, 128, 32, 32]             256\n",
      "             ReLU-51          [-1, 128, 32, 32]               0\n",
      "           Conv2d-52          [-1, 128, 32, 32]         147,456\n",
      "      BatchNorm2d-53          [-1, 128, 32, 32]             256\n",
      "             ReLU-54          [-1, 128, 32, 32]               0\n",
      "       BasicBlock-55          [-1, 128, 32, 32]               0\n",
      "           Conv2d-56          [-1, 256, 16, 16]         294,912\n",
      "      BatchNorm2d-57          [-1, 256, 16, 16]             512\n",
      "             ReLU-58          [-1, 256, 16, 16]               0\n",
      "           Conv2d-59          [-1, 256, 16, 16]         589,824\n",
      "      BatchNorm2d-60          [-1, 256, 16, 16]             512\n",
      "           Conv2d-61          [-1, 256, 16, 16]          32,768\n",
      "      BatchNorm2d-62          [-1, 256, 16, 16]             512\n",
      "             ReLU-63          [-1, 256, 16, 16]               0\n",
      "       BasicBlock-64          [-1, 256, 16, 16]               0\n",
      "           Conv2d-65          [-1, 256, 16, 16]         589,824\n",
      "      BatchNorm2d-66          [-1, 256, 16, 16]             512\n",
      "             ReLU-67          [-1, 256, 16, 16]               0\n",
      "           Conv2d-68          [-1, 256, 16, 16]         589,824\n",
      "      BatchNorm2d-69          [-1, 256, 16, 16]             512\n",
      "             ReLU-70          [-1, 256, 16, 16]               0\n",
      "       BasicBlock-71          [-1, 256, 16, 16]               0\n",
      "           Conv2d-72          [-1, 256, 16, 16]         589,824\n",
      "      BatchNorm2d-73          [-1, 256, 16, 16]             512\n",
      "             ReLU-74          [-1, 256, 16, 16]               0\n",
      "           Conv2d-75          [-1, 256, 16, 16]         589,824\n",
      "      BatchNorm2d-76          [-1, 256, 16, 16]             512\n",
      "             ReLU-77          [-1, 256, 16, 16]               0\n",
      "       BasicBlock-78          [-1, 256, 16, 16]               0\n",
      "           Conv2d-79          [-1, 256, 16, 16]         589,824\n",
      "      BatchNorm2d-80          [-1, 256, 16, 16]             512\n",
      "             ReLU-81          [-1, 256, 16, 16]               0\n",
      "           Conv2d-82          [-1, 256, 16, 16]         589,824\n",
      "      BatchNorm2d-83          [-1, 256, 16, 16]             512\n",
      "             ReLU-84          [-1, 256, 16, 16]               0\n",
      "       BasicBlock-85          [-1, 256, 16, 16]               0\n",
      "           Conv2d-86          [-1, 256, 16, 16]         589,824\n",
      "      BatchNorm2d-87          [-1, 256, 16, 16]             512\n",
      "             ReLU-88          [-1, 256, 16, 16]               0\n",
      "           Conv2d-89          [-1, 256, 16, 16]         589,824\n",
      "      BatchNorm2d-90          [-1, 256, 16, 16]             512\n",
      "             ReLU-91          [-1, 256, 16, 16]               0\n",
      "       BasicBlock-92          [-1, 256, 16, 16]               0\n",
      "           Conv2d-93          [-1, 256, 16, 16]         589,824\n",
      "      BatchNorm2d-94          [-1, 256, 16, 16]             512\n",
      "             ReLU-95          [-1, 256, 16, 16]               0\n",
      "           Conv2d-96          [-1, 256, 16, 16]         589,824\n",
      "      BatchNorm2d-97          [-1, 256, 16, 16]             512\n",
      "             ReLU-98          [-1, 256, 16, 16]               0\n",
      "       BasicBlock-99          [-1, 256, 16, 16]               0\n",
      "          Conv2d-100            [-1, 512, 8, 8]       1,179,648\n",
      "     BatchNorm2d-101            [-1, 512, 8, 8]           1,024\n",
      "            ReLU-102            [-1, 512, 8, 8]               0\n",
      "          Conv2d-103            [-1, 512, 8, 8]       2,359,296\n",
      "     BatchNorm2d-104            [-1, 512, 8, 8]           1,024\n",
      "          Conv2d-105            [-1, 512, 8, 8]         131,072\n",
      "     BatchNorm2d-106            [-1, 512, 8, 8]           1,024\n",
      "            ReLU-107            [-1, 512, 8, 8]               0\n",
      "      BasicBlock-108            [-1, 512, 8, 8]               0\n",
      "          Conv2d-109            [-1, 512, 8, 8]       2,359,296\n",
      "     BatchNorm2d-110            [-1, 512, 8, 8]           1,024\n",
      "            ReLU-111            [-1, 512, 8, 8]               0\n",
      "          Conv2d-112            [-1, 512, 8, 8]       2,359,296\n",
      "     BatchNorm2d-113            [-1, 512, 8, 8]           1,024\n",
      "            ReLU-114            [-1, 512, 8, 8]               0\n",
      "      BasicBlock-115            [-1, 512, 8, 8]               0\n",
      "          Conv2d-116            [-1, 512, 8, 8]       2,359,296\n",
      "     BatchNorm2d-117            [-1, 512, 8, 8]           1,024\n",
      "            ReLU-118            [-1, 512, 8, 8]               0\n",
      "          Conv2d-119            [-1, 512, 8, 8]       2,359,296\n",
      "     BatchNorm2d-120            [-1, 512, 8, 8]           1,024\n",
      "            ReLU-121            [-1, 512, 8, 8]               0\n",
      "      BasicBlock-122            [-1, 512, 8, 8]               0\n",
      "          Conv2d-123            [-1, 512, 8, 8]       2,359,808\n",
      "          Conv2d-124            [-1, 512, 8, 8]       2,359,808\n",
      "          Conv2d-125            [-1, 512, 8, 8]       2,359,808\n",
      "          Conv2d-126            [-1, 512, 8, 8]       2,359,808\n",
      "          Dblock-127            [-1, 512, 8, 8]               0\n",
      "          Conv2d-128            [-1, 128, 8, 8]          65,664\n",
      "     BatchNorm2d-129            [-1, 128, 8, 8]             256\n",
      " ConvTranspose2d-130          [-1, 128, 16, 16]         147,584\n",
      "     BatchNorm2d-131          [-1, 128, 16, 16]             256\n",
      "          Conv2d-132          [-1, 256, 16, 16]          33,024\n",
      "     BatchNorm2d-133          [-1, 256, 16, 16]             512\n",
      "    DecoderBlock-134          [-1, 256, 16, 16]               0\n",
      "          Conv2d-135           [-1, 64, 16, 16]          16,448\n",
      "     BatchNorm2d-136           [-1, 64, 16, 16]             128\n",
      " ConvTranspose2d-137           [-1, 64, 32, 32]          36,928\n",
      "     BatchNorm2d-138           [-1, 64, 32, 32]             128\n",
      "          Conv2d-139          [-1, 128, 32, 32]           8,320\n",
      "     BatchNorm2d-140          [-1, 128, 32, 32]             256\n",
      "    DecoderBlock-141          [-1, 128, 32, 32]               0\n",
      "          Conv2d-142           [-1, 32, 32, 32]           4,128\n",
      "     BatchNorm2d-143           [-1, 32, 32, 32]              64\n",
      " ConvTranspose2d-144           [-1, 32, 64, 64]           9,248\n",
      "     BatchNorm2d-145           [-1, 32, 64, 64]              64\n",
      "          Conv2d-146           [-1, 64, 64, 64]           2,112\n",
      "     BatchNorm2d-147           [-1, 64, 64, 64]             128\n",
      "    DecoderBlock-148           [-1, 64, 64, 64]               0\n",
      "          Conv2d-149           [-1, 16, 64, 64]           1,040\n",
      "     BatchNorm2d-150           [-1, 16, 64, 64]              32\n",
      " ConvTranspose2d-151         [-1, 16, 128, 128]           2,320\n",
      "     BatchNorm2d-152         [-1, 16, 128, 128]              32\n",
      "          Conv2d-153         [-1, 64, 128, 128]           1,088\n",
      "     BatchNorm2d-154         [-1, 64, 128, 128]             128\n",
      "    DecoderBlock-155         [-1, 64, 128, 128]               0\n",
      " ConvTranspose2d-156         [-1, 32, 256, 256]          32,800\n",
      "          Conv2d-157         [-1, 32, 256, 256]           9,248\n",
      "          Conv2d-158          [-1, 1, 256, 256]             289\n",
      "================================================================\n",
      "Total params: 31,096,129\n",
      "Trainable params: 31,096,129\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.75\n",
      "Forward/backward pass size (MB): 203.38\n",
      "Params size (MB): 118.62\n",
      "Estimated Total Size (MB): 322.75\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1639: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    }
   ],
   "source": [
    "summary(net.cuda(), (3, 256, 256))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dice_bce_loss(nn.Module):\n",
    "    def __init__(self, batch=True):\n",
    "        super(dice_bce_loss, self).__init__()\n",
    "        self.batch = batch\n",
    "        self.bce_loss = nn.BCELoss()\n",
    "        \n",
    "    def soft_dice_coeff(self, y_true, y_pred):\n",
    "        smooth = 0.0  # may change\n",
    "        if self.batch:\n",
    "            i = torch.sum(y_true)\n",
    "            j = torch.sum(y_pred)\n",
    "            intersection = torch.sum(y_true * y_pred)\n",
    "        else:\n",
    "            i = y_true.sum(1).sum(1).sum(1)\n",
    "            j = y_pred.sum(1).sum(1).sum(1)\n",
    "            intersection = (y_true * y_pred).sum(1).sum(1).sum(1)\n",
    "        score = (2. * intersection + smooth) / (i + j + smooth)\n",
    "        #score = (intersection + smooth) / (i + j - intersection + smooth)#iou\n",
    "        return score.mean()\n",
    "\n",
    "    def soft_dice_loss(self, y_true, y_pred):\n",
    "        loss = 1 - self.soft_dice_coeff(y_true, y_pred)\n",
    "        return loss\n",
    "        \n",
    "    def __call__(self, y_true, y_pred):\n",
    "        a =  self.bce_loss(y_pred, y_true)\n",
    "        b =  self.soft_dice_loss(y_true, y_pred)\n",
    "        return a + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 7056 images\n",
      "Read 224 images\n"
     ]
    }
   ],
   "source": [
    "train_dataset = data_loader.RoadDataset(road_path, True, INPUT_SIZE, OUTPUT_SIZE)\n",
    "val_dataset = data_loader.RoadDataset(road_path, False, INPUT_SIZE, OUTPUT_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, BATCH_SIZE, shuffle = False)\n",
    "val_loader = DataLoader(val_dataset, batch_size = BATCH_SIZE, shuffle = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiceCoeff(Function):\n",
    "    \"\"\"Dice coeff for individual examples\"\"\"\n",
    "\n",
    "    # 在进入 forward 之前，所有变量都会被转化为 tensor\n",
    "    def forward(self, input, target):\n",
    "        self.save_for_backward(input, target) # tensor 转化为变量保存到后续操作\n",
    "        eps = 0.0001\n",
    "        self.inter = torch.dot(input.view(-1), target.view(-1))\n",
    "        self.union = torch.sum(input) + torch.sum(target) + eps\n",
    "\n",
    "        t = (2 * self.inter.float() + eps) / self.union.float()\n",
    "        return t\n",
    "\n",
    "    # This function has only a single output, so it gets only one gradient\n",
    "    def backward(self, grad_output):\n",
    "        input, target = self.saved_variables\n",
    "        grad_input = grad_target = None\n",
    "\n",
    "        # 判断 input 是否需要求梯度\n",
    "        if self.needs_input_grad[0]:\n",
    "            grad_input = grad_output * 2 * (target * self.union - self.inter) \\\n",
    "                         / (self.union * self.union)\n",
    "        # 判断 target 是否需要求梯度\n",
    "        if self.needs_input_grad[1]:\n",
    "            grad_target = None\n",
    "\n",
    "        return grad_input, grad_target\n",
    "\n",
    "\n",
    "def dice_coeff(input, target):\n",
    "    \"\"\"Dice coeff for batches\"\"\"\n",
    "    # 在合适的设备上初始化一个1*1零向量\n",
    "    # 同一个 batch 中 dice loss 取平均\n",
    "    s = torch.FloatTensor(1).cuda().zero_() if input.is_cuda else torch.FloatTensor(1).zero_()\n",
    "    for i, c in enumerate(zip(input, target)):\n",
    "        s = s + DiceCoeff().forward(c[0], c[1])\n",
    "    return s / (i + 1)\n",
    "\n",
    "def eval_net(net, loader, device):\n",
    "    \"\"\"Evaluation without the densecrf with the dice coefficient\"\"\"\n",
    "    # 关闭 batchnorm 和 dropout\n",
    "    net.eval() # 仔细看\n",
    "    mask_type = torch.float32 if net.n_classes == 1 else torch.long\n",
    "    n_val = len(loader)  # the number of batch\n",
    "    tot = 0\n",
    "\n",
    "    # 括号里设置文字输出信息\n",
    "#     with tqdm(total = n_val, desc='Validation round', unit='batch', leave = False) as pbar:\n",
    "        # 对于每个 batch\n",
    "    for batch in loader:\n",
    "        imgs, true_masks = batch[0], batch[1]\n",
    "        imgs = imgs.to(device=device, dtype=torch.float32)\n",
    "        true_masks = true_masks.to(device=device, dtype=mask_type)\n",
    "\n",
    "        # 不需要追踪梯度变化，不需要进行反向传播，提升速度\n",
    "        with torch.no_grad():\n",
    "            # 得到模型预测结果\n",
    "            mask_pred = net(imgs)\n",
    "\n",
    "        # 不同类别预测结果损失累加\n",
    "        if net.n_classes > 1:\n",
    "            tot += F.cross_entropy(mask_pred, true_masks).item()\n",
    "        else:\n",
    "            pred = torch.sigmoid(mask_pred)\n",
    "            pred = (pred > 0.5).float()\n",
    "            tot += dice_coeff(pred, true_masks).item()\n",
    "#             pbar.update()\n",
    "\n",
    "    net.train()\n",
    "    return tot / n_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_net(net, device, train_dataset, val_dataset, optimizer, epochs = EPOCH_NUM, lr = LR, save_cp = True,\n",
    "             batch_size = BATCH_SIZE):\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, BATCH_SIZE, shuffle = False)\n",
    "    val_loader = DataLoader(val_dataset, batch_size = BATCH_SIZE, shuffle = False)\n",
    "    \n",
    "    # 每轮 evaluation 检验的 batch 个数\n",
    "    n_val = len(val_dataset)\n",
    "    # 每轮 train 检验的 batch 个数\n",
    "    n_train = len(train_dataset)\n",
    "\n",
    "    writer = SummaryWriter(comment=f'LR_{lr}_BS_{BATCH_SIZE}')\n",
    "    global_step = 0\n",
    "\n",
    "    logging.info(f'''Starting training:\n",
    "        Epochs:          {epochs}\n",
    "        Batch size:      {batch_size}\n",
    "        Learning rate:   {lr}\n",
    "        Training size:   {n_train}\n",
    "        Validation size: {n_val}\n",
    "        Checkpoints:     {save_cp}\n",
    "        Device:          {device.type}\n",
    "    ''')\n",
    "#     换 SGD，图像用 SGD Adam，收敛速度而非效果\n",
    "#     optimizer = optim.RMSprop(net.parameters(), lr=lr, weight_decay=1e-8, momentum=0.9)\n",
    "#     optimizer = optim.SGD(net.parameters(), lr=lr, weight_decay=1e-8, momentum=0.9)\n",
    "#     optimizer = optim.Adam(params = net.parameters(), lr = lr)\n",
    "    # 在发现loss不再降低或者acc不再提高之后，降低学习率。patience 含义：不再减小（或增大）的累计次数\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min' if net.n_classes > 1 else 'max', patience=2)\n",
    "    \n",
    "    if net.n_classes > 1:\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "    else:\n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        net.train()\n",
    "        print(net.firstconv.weight[0,0,0])\n",
    "\n",
    "        epoch_loss = 0\n",
    "        with tqdm(total = n_train, desc=f'Epoch {epoch + 1}/{epochs}', unit='img') as pbar:\n",
    "            for batch in train_loader:\n",
    "                \n",
    "                imgs = batch[0]\n",
    "                true_masks = batch[1]\n",
    "\n",
    "                imgs = imgs.to(device = device, dtype = torch.float32)\n",
    "                mask_type = torch.float32 if net.n_classes == 1 else torch.long\n",
    "                true_masks = true_masks.to(device = device, dtype = mask_type) # 01\n",
    "\n",
    "                masks_pred = net(imgs)\n",
    "                loss = criterion(masks_pred, true_masks)\n",
    "                epoch_loss += loss.item()\n",
    "                writer.add_scalar('Loss/train', loss.item(), global_step)\n",
    "\n",
    "                pbar.set_postfix(**{'loss (batch)': loss.item()})\n",
    "\n",
    "                # 对于每个 batch 都要更新一次参数空间\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                # 防止梯度爆炸，设置梯度截断\n",
    "                nn.utils.clip_grad_value_(net.parameters(), 1)\n",
    "                optimizer.step()\n",
    "\n",
    "                # 每个 batch 结束更新一次进度条，迭代器内部计数器累加 batch 的大小\n",
    "                pbar.update(imgs.shape[0])\n",
    "                global_step += 1\n",
    "                \n",
    "                # 在 tensorboard 中记录一次\n",
    "                if global_step % (n_train // (10 * batch_size) + 1) == 0:\n",
    "                    for tag, value in net.named_parameters():\n",
    "                        tag = tag.replace('.', '/')\n",
    "                        writer.add_histogram('weights/' + tag, value.data.cpu().numpy(), global_step)\n",
    "                        writer.add_histogram('grads/' + tag, value.grad.data.cpu().numpy(), global_step)\n",
    "                    val_score = eval_net(net, val_loader, device)\n",
    "                    scheduler.step(val_score)\n",
    "                    writer.add_scalar('learning_rate', optimizer.param_groups[0]['lr'], global_step)\n",
    "\n",
    "                    if net.n_classes > 1:\n",
    "                        logging.info('Validation cross entropy: {}'.format(val_score))\n",
    "                        writer.add_scalar('Loss/test', val_score, global_step)\n",
    "                    else:\n",
    "                        logging.info('Validation Dice Coeff: {}'.format(val_score))\n",
    "                        writer.add_scalar('Dice/test', val_score, global_step)\n",
    "\n",
    "                    writer.add_images('images', imgs, global_step)\n",
    "                    if net.n_classes == 1:\n",
    "                        writer.add_images('masks/true', true_masks, global_step)\n",
    "                        writer.add_images('masks/pred_0.5', torch.sigmoid(masks_pred) > 0.5, global_step)\n",
    "                        writer.add_images('masks/pred_0.4', torch.sigmoid(masks_pred) > 0.4, global_step)\n",
    "                        writer.add_images('masks/pred_0.3', torch.sigmoid(masks_pred) > 0.3, global_step)\n",
    "                        writer.add_images('masks/pred_0.2', torch.sigmoid(masks_pred) > 0.2, global_step)\n",
    "                        writer.add_images('masks/pred_0.1', torch.sigmoid(masks_pred) > 0.1, global_step)\n",
    "\n",
    "        if save_cp:\n",
    "            try:\n",
    "                os.mkdir(DIR_CHECKPOINT)\n",
    "                logging.info('Created checkpoint directory')\n",
    "            except OSError:\n",
    "                pass\n",
    "            torch.save(net.state_dict(),\n",
    "                       DIR_CHECKPOINT + f'dlinknet_epoch{epoch + 1}.pth')\n",
    "            logging.info(f'Checkpoint {epoch + 1} saved.')\n",
    "            if os.path.exists(DIR_CHECKPOINT + f'dlinknet_epoch{epoch - 4}.pth') & (epoch - 4)//10 != 0:\n",
    "                os.remove(DIR_CHECKPOINT + f'dlinknet_epoch{epoch - 4}.pth')\n",
    "                logging.info(f'Checkpoint {epoch - 4} deleted.')\n",
    "\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_printoptions(precision=8)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "net = DLinkNet34().to(device)\n",
    "optimizer = optim.Adam(params = net.parameters(), lr = LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/20:   0%|          | 0/7056 [00:00<?, ?img/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.00541094, -0.00690917,  0.00788385,  0.03791069,  0.04907195,\n",
      "         0.03065980,  0.02539830], device='cuda:0', grad_fn=<SelectBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1639: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "Epoch 1/20: 100%|██████████| 7056/7056 [22:39<00:00,  5.19img/s, loss (batch)=0.666]  \n",
      "Epoch 2/20:   0%|          | 0/7056 [00:00<?, ?img/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.00309607, -0.00826241,  0.01103375,  0.03800877,  0.05120587,\n",
      "         0.02281113,  0.02068197], device='cuda:0', grad_fn=<SelectBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20: 100%|██████████| 7056/7056 [22:18<00:00,  5.27img/s, loss (batch)=0.667]  \n",
      "Epoch 3/20:   0%|          | 0/7056 [00:00<?, ?img/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.00343992, -0.00820112,  0.01095000,  0.03785765,  0.05088929,\n",
      "         0.02286792,  0.02099382], device='cuda:0', grad_fn=<SelectBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/20: 100%|██████████| 7056/7056 [22:25<00:00,  5.24img/s, loss (batch)=0.667]  \n",
      "Epoch 4/20:   0%|          | 0/7056 [00:00<?, ?img/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.00343784, -0.00820306,  0.01094818,  0.03785596,  0.05088766,\n",
      "         0.02286675,  0.02099283], device='cuda:0', grad_fn=<SelectBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/20: 100%|██████████| 7056/7056 [22:26<00:00,  5.24img/s, loss (batch)=0.667]  \n",
      "Epoch 5/20:   0%|          | 0/7056 [00:00<?, ?img/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.00343763, -0.00820318,  0.01094811,  0.03785600,  0.05088773,\n",
      "         0.02286674,  0.02099280], device='cuda:0', grad_fn=<SelectBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/20:  21%|██▏       | 1512/7056 [04:42<12:28,  7.41img/s, loss (batch)=0.666] "
     ]
    }
   ],
   "source": [
    "train_net(net, device, train_dataset, val_dataset, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
